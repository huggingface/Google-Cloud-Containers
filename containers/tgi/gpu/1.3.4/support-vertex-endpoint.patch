diff --git a/router/src/lib.rs b/router/src/lib.rs
index 898fcd0..b236443 100644
--- a/router/src/lib.rs
+++ b/router/src/lib.rs
@@ -11,6 +11,24 @@ use serde::{Deserialize, Serialize};
 use utoipa::ToSchema;
 use validation::Validation;
 
+#[derive(Clone, Deserialize, ToSchema)]
+pub(crate) struct Instance {
+    pub inputs: String,
+    pub parameters: GenerateParameters,
+}
+
+#[derive(Deserialize, ToSchema)]
+pub(crate) struct VertexRequest {
+    pub instances: Vec<Instance>,
+    #[allow(dead_code)]
+    pub parameters: Option<GenerateParameters>,
+}
+
+#[derive(Clone, Deserialize, ToSchema, Serialize)]
+pub(crate) struct VertexResponse {
+    pub predictions: Vec<String>,
+}
+
 /// Hub type
 #[derive(Clone, Debug, Deserialize)]
 pub struct HubModelInfo {
diff --git a/router/src/main.rs b/router/src/main.rs
index 4637c77..4194088 100644
--- a/router/src/main.rs
+++ b/router/src/main.rs
@@ -166,7 +166,7 @@ async fn main() -> Result<(), RouterError> {
             .with_progress(false)
             .with_token(authorization_token);
 
-        if let Some(cache_dir) = std::env::var("HUGGINGFACE_HUB_CACHE").ok() {
+        if let Ok(cache_dir) = std::env::var("HUGGINGFACE_HUB_CACHE") {
             builder = builder.with_cache_dir(cache_dir.into());
         }
 
diff --git a/router/src/server.rs b/router/src/server.rs
index 3db5c7c..9361415 100644
--- a/router/src/server.rs
+++ b/router/src/server.rs
@@ -5,7 +5,7 @@ use crate::validation::ValidationError;
 use crate::{
     BestOfSequence, CompatGenerateRequest, Details, ErrorResponse, FinishReason,
     GenerateParameters, GenerateRequest, GenerateResponse, HubModelInfo, Infer, Info, PrefillToken,
-    StreamDetails, StreamResponse, Token, Validation,
+    StreamDetails, StreamResponse, Token, Validation, VertexRequest, VertexResponse,
 };
 use axum::extract::Extension;
 use axum::http::{HeaderMap, Method, StatusCode};
@@ -14,8 +14,10 @@ use axum::response::{IntoResponse, Response};
 use axum::routing::{get, post};
 use axum::{http, Json, Router};
 use axum_tracing_opentelemetry::middleware::OtelAxumLayer;
+use futures::stream::FuturesUnordered;
 use futures::stream::StreamExt;
 use futures::Stream;
+use futures::TryStreamExt;
 use metrics_exporter_prometheus::{Matcher, PrometheusBuilder, PrometheusHandle};
 use std::convert::Infallible;
 use std::net::SocketAddr;
@@ -503,6 +505,107 @@ async fn generate_stream(
     (headers, Sse::new(stream).keep_alive(KeepAlive::default()))
 }
 
+/// Generate tokens
+#[utoipa::path(
+    post,
+    tag = "Text Generation Inference",
+    path = "/v1/endpoint",
+    request_body = VertexRequest,
+    responses(
+    (status = 200, description = "Generated Text", body = GenerateResponse),
+    (status = 424, description = "Generation Error", body = ErrorResponse,
+    example = json ! ({"error": "Request failed during generation"})),
+    (status = 429, description = "Model is overloaded", body = ErrorResponse,
+    example = json ! ({"error": "Model is overloaded"})),
+    (status = 422, description = "Input validation error", body = ErrorResponse,
+    example = json ! ({"error": "Input validation error"})),
+    (status = 500, description = "Incomplete generation", body = ErrorResponse,
+    example = json ! ({"error": "Incomplete generation"})),
+    )
+    )]
+#[instrument(
+    skip_all,
+    fields(
+    // parameters = ? req.parameters,
+    total_time,
+    validation_time,
+    queue_time,
+    inference_time,
+    time_per_token,
+    seed,
+    )
+    )]
+async fn vertex_compatibility(
+    Extension(infer): Extension<Infer>,
+    Json(req): Json<VertexRequest>,
+) -> Result<Response, (StatusCode, Json<ErrorResponse>)> {
+    metrics::increment_counter!("tgi_request_count");
+
+    // check that theres at least one instance
+    if req.instances.is_empty() {
+        return Err((
+            StatusCode::UNPROCESSABLE_ENTITY,
+            Json(ErrorResponse {
+                error: "Input validation error".to_string(),
+                error_type: "Input validation error".to_string(),
+            }),
+        ));
+    }
+
+    // Process all instances
+    let predictions = req
+        .instances
+        .iter()
+        .map(|instance| {
+            let inputs = &instance.inputs;
+            let parameters = &instance.parameters;
+
+            let generate_request = GenerateRequest {
+                inputs: inputs.clone(),
+                parameters: GenerateParameters {
+                    do_sample: true,
+                    max_new_tokens: parameters.max_new_tokens,
+                    seed: parameters.seed,
+                    best_of: None,
+                    temperature: None,
+                    repetition_penalty: None,
+                    top_k: None,
+                    top_p: None,
+                    typical_p: None,
+                    return_full_text: None,
+                    stop: Vec::new(),
+                    truncate: None,
+                    watermark: false,
+                    details: true,
+                    decoder_input_details: true,
+                    top_n_tokens: None,
+                },
+            };
+
+            async {
+                generate(Extension(infer.clone()), Json(generate_request))
+                    .await
+                    .map(|(_, Json(generation))| generation.generated_text)
+                    .map_err(|_| {
+                        (
+                            StatusCode::INTERNAL_SERVER_ERROR,
+                            Json(ErrorResponse {
+                                error: "Incomplete generation".to_string(),
+                                error_type: "Incomplete generation".to_string(),
+                            }),
+                        )
+                    })
+            }
+        })
+        .collect::<FuturesUnordered<_>>()
+        .try_collect::<Vec<_>>()
+        .await?;
+
+    // Create and send response
+    let response = VertexResponse { predictions };
+    Ok((HeaderMap::new(), Json(response)).into_response())
+}
+
 /// Prometheus metrics scrape endpoint
 #[utoipa::path(
 get,
@@ -693,6 +796,7 @@ pub async fn run(
         .route("/info", get(get_model_info))
         .route("/generate", post(generate))
         .route("/generate_stream", post(generate_stream))
+        .route("/v1/endpoint", post(vertex_compatibility))
         // AWS Sagemaker route
         .route("/invocations", post(compat_generate))
         // Base Health route
