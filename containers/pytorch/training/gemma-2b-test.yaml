# Model parameters for training
model_name_or_path: google/gemma-2b
attn_implementation: flash_attention_2
torch_dtype: bfloat16
# Dataset parameters
dataset_name: OpenAssistant/oasst_top1_2023-08-25
dataset_text_field: text
# Training parameters
num_train_epochs: 1
learning_rate: 0.0002
logging_steps: 10
bf16: True
tf32: True
per_device_train_batch_size: 4
use_peft: True
load_in_4bit: True
output_dir: /artifacts