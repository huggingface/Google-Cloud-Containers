{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553c8548-0a0a-4dc9-ae22-1f9adefedb50",
   "metadata": {},
   "source": [
    "## Finetune Gemma-2B To A General Purpose Chatbot using ðŸ¤— peft, trl, bitsandbytes & transformers\n",
    "\n",
    "This notebook runs on top of the image built using this Dockerfile:\n",
    "[GitHub Link](https://github.com/huggingface/Google-Cloud-Containers/blob/main/containers/pytorch/training/gpu/2.1/transformers/4.38.0.dev0/py310/Dockerfile)\n",
    "\n",
    "Using this image you don't need to install any packages, as all needed packages are already there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8cb33c-8042-4110-bc57-9544a59001da",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. As the model weights are still in a private organization on HuggingFace Hub, you need to authenticate yourself in order to download model weights. You can use this from CLI:\n",
    "    ```bash\n",
    "    huggingface-cli login\n",
    "    ```\n",
    "    There are other ways too which can be found [here](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284deca-dec1-4672-a87f-b3663c34b81a",
   "metadata": {},
   "source": [
    "### Import libraries and specify model to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6eb6a17-7ab9-4156-bcd5-63877dfd3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, GemmaTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf1aae5-a852-4442-8f14-faedd2efdb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the 2b model for demonstration\n",
    "model_id = \"gg-hf/gemma-2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9116241-c1a3-4af3-a6cc-baf268beb8a8",
   "metadata": {},
   "source": [
    "### Load the dataset for training\n",
    "\n",
    "We use the [Guanaco dataset](https://huggingface.co/datasets/timdettmers/openassistant-guanaco), a refined part of the OpenAssistant dataset designed specifically to train versatile chatbots. The dataset contains various questions that require generative outputs.\n",
    "\n",
    "The data is like a question along with its answer. Further, its multi-lingual, i.e., we have questions in English and in Spanish. The dataset contains about 9.85K training instances along with 518 test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3259e5fb-bc76-42d7-a58b-e844b8ec07ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library for loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the name of the dataset\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "# Load the dataset from the specified name and select the \"train\" split\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1b237-8c75-4914-8726-9b48ab15fe79",
   "metadata": {},
   "source": [
    "### Load Quantized model using bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5e5e44-744d-45fb-ab85-26a7ce0d994f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #  quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\", #use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_use_double_quant=True, #use a nested quantization scheme to quantize the already quantized weights\n",
    "    bnb_4bit_compute_dtype=torch.float16, #for faster computation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bea530-d950-4c52-8109-d82ee79edeec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc7ef8-3e44-43d8-b782-fca2de0a7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a05f22-4d15-4a16-9d01-675db096fc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ee3ac-d4cf-46e5-a0dc-b36e54f0a6dc",
   "metadata": {},
   "source": [
    "### Initlializer configuration file to construct the LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6028720e-409d-4bd1-9daa-d769797ee8be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"Causal_LM\", \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"o_proj\", \"v_proj\"], # We get the value from the Module List when we printed the model object\n",
    "    inference_mode=False, \n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff52d58-7568-42f6-a9e1-68ab6c1fe5c9",
   "metadata": {},
   "source": [
    "### Train with SFTTrainer\n",
    "\n",
    "It is provided by the [TRL](https://huggingface.co/docs/trl/index) library, which offers a convenient interface around the Transformers Trainer and enables straightforward supervised fine-tuning of models on instruction-based datasets using PEFT adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7baef6eb-e448-48f0-ac49-32369249e50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a54a0a-0314-4c76-b35f-f694da5ec097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9846/9846 [00:02<00:00, 3615.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 5209.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#### Define arguments for training and then \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4, #  batch size per device during training\n",
    "    gradient_accumulation_steps = 4, # Number of steps to accumulate gradients before updating the model\n",
    "    max_grad_norm = 0.3, # Maximum gradient norm for gradient clipping\n",
    "    warmup_ratio = 0.03, # # Warmup ratio for learning rate scheduling\n",
    "    lr_scheduler_type = \"constant\", # Type of learning rate scheduler \n",
    "    max_steps = 100, # Maximum number of training steps\n",
    "    logging_steps=10, # Interval to log training metrics\n",
    "    group_by_length=True,\n",
    "    eval_steps=20, # Evaluate every n steps during training\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\", # Column/field that contains the text in the dataset\n",
    "    max_seq_length=512, # Set the maximum sequence length\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5b7a60-c8a3-4730-a95d-ea7920e46b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 15:03, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.677100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.819800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.8983008775711059, metrics={'train_runtime': 905.0472, 'train_samples_per_second': 17.679, 'train_steps_per_second': 1.105, 'total_flos': 5.615236195757261e+16, 'train_loss': 1.8983008775711059, 'epoch': 1.62})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [huggingface-pytorch-training-gpu.2.1.transformers.4.37.2.py310] (Local)",
   "language": "python",
   "name": "us-central1-docker.pkg.dev_gcp-partnership-412108_deep-learning-images_huggingface-pytorch-training-gpu.2.1.transformers.4.37.2.py310_sha256_027f7ecb1ad8fbe9804b877aa22ba2e3939c964e8bac0283706adf899a780841__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
