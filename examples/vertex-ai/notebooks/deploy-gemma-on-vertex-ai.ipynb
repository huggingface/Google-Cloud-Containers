{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Gemma 7B on Google Vertex AI \n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-gemma-on-vertex-ai.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "[Gemma-7b](https://huggingface.co/google/gemma-7b) is state-of-the-art open model from Google, built from the same research and technology used to create the Gemini models. It is a text-to-text, decoder-only large language model, available in English, with open weights, and is really well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Learn more about it [Welcome Gemma - Googleâ€™s new open LLM](https://huggingface.co/blog/gemma).\n",
    "\n",
    "In this tutorial you will learn how to deploy [google/gemma-7b](https://huggingface.co/google/gemma-7b) on Vertex AI Endpoints. We are going to use the Hugging Face [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) container which is a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "What you'll learn in this blog:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Configure gcloud CLI](#2-configure-gcloud-cli)\n",
    "3. [Initialize Vertex AI SDK](#3-initialize-vertex-ai-sdk)\n",
    "4. [Deploy Gemma-7B on Vertex AI](#4-deploy-gemma-7b-on-vertex-ai)\n",
    "5. [Run Inference with deployed Model](#5-run-inference-with-deployed-model)\n",
    "6. [Cleaning Up Resources](#6-cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `Vertex AI` python SDK to deploy [google/gemma-7b](https://huggingface.co/google/gemma-7b) on Vertex AI. You need to have a GCP project and an account with the necessary permissions to create resources in the project. \n",
    "\n",
    "Before we can install the packages we need to install the `gcloud CLI`. Instructions can be found here: https://cloud.google.com/sdk/docs/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages for the notebook\n",
    "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-storage \"google-auth>=2.23.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure gcloud CLI\n",
    "\n",
    "We need to authenticate with Google Cloud SDK to use the Vertex AI services. Run the following command to authenticate with Google Cloud SDK.\n",
    "\n",
    "```bash\n",
    "gcloud auth login \n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gcp-project-id\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "BUCKET_URI = f\"gs://vertexai-{PROJECT_ID}-tgi\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID} --quiet\n",
    "# Set the region\n",
    "! gcloud config set ai/region {REGION} --quiet\n",
    "# create the bucket if it doesn't exist\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Vertex AI SDK \n",
    "\n",
    "The following set of constants will be used to create names and display names of Vertex AI Prediction resources like models, endpoints, and model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model names and version\n",
    "MODEL_NAME = \"Gemma-7b\" # @param {type:\"string\"}\n",
    "MODEL_VERSION = \"v01\" # @param {type: \"string\"}\n",
    "MODEL_DISPLAY_NAME = f\"TGI-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = f\"endpoint-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "\n",
    "# Set the TGI serving container image uri, selected from https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face\n",
    "SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.1-4.ubuntu2204.py310\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Gemma-7B on Vertex AI\n",
    "\n",
    "To deploy [Gemma-7B](https://huggingface.co/google/gemma-7b) on Vertex AI, we first need to first upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/755607090520/locations/us-central1/models/8539037099238621184/operations/3099102200106844160\n",
      "Model created. Resource name: projects/755607090520/locations/us-central1/models/8539037099238621184@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/755607090520/locations/us-central1/models/8539037099238621184@1')\n",
      "TGI-Gemma-7b-v01\n",
      "projects/755607090520/locations/us-central1/models/8539037099238621184\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"google/gemma-7b\", # Hugging Face model ID\n",
    "        \"NUM_SHARD\": \"1\", \n",
    "        \"MAX_INPUT_LENGTH\": \"512\", \n",
    "        \"MAX_TOTAL_TOKENS\": \"1024\", \n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": \"1512\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": \"TOKEN WITH ACCESS TO Gemma\", # Replace with your Hugging Face Hub token\n",
    "        },\n",
    "    serving_container_ports=[80],\n",
    ")\n",
    "\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is uploaded, now we can deploy it on Vertex AI Endpoints. \n",
    "First, we need to create an endpoint and then deploy the model to the endpoint. Here you also need to choose the hardware configuration for the deployment. \n",
    "\n",
    "The deployment will take ~20-25 minutes. You can check the status of the deployment in the Google cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/755607090520/locations/us-central1/endpoints/8199922424465063936/operations/3741428096960561152\n",
      "Endpoint created. Resource name: projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/755607090520/locations/us-central1/endpoints/8199922424465063936')\n",
      "Deploying model to Endpoint : projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n",
      "Deploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/8199922424465063936/operations/8404905511102709760\n",
      "Endpoint model deployed. Resource name: projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n"
     ]
    }
   ],
   "source": [
    "machine_type = 'g2-standard-4' # L4 GPUs\n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_NAME, # The display name of the deployed model\n",
    "    machine_type=machine_type,              # type of machine, read more here: https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "    accelerator_type=\"NVIDIA_L4\",           # Hardware accelerator \n",
    "    accelerator_count=1,                    # Number of accelerators to attach to a worker replica.\n",
    "    traffic_percentage=100,                 # Percentage of traffic to send to this model\n",
    "    min_replica_count=1,                    # The minimum number of machine replicas this deployed model will be always deployed on.\n",
    "    sync=True,                              # Whether to execute this method synchronously.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Inference with deployed Model\n",
    "Awesome! We have successfully deployed the Gemma-7B model on Vertex AI. Now let's run inference on our endpoint. We will use the `predict` method of the `Endpoint` class to run inference on the deployed model. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [TGI documentation](https://huggingface.github.io/text-generation-inference/) defined under the `GenerateParameters` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning is an advanced machine learning technique used in automated, semi-automated, and manual testing. It can achieve higher accuracy through training rather than developing algorithms; data representations based on neural networks are used.\n",
      "\n",
      "Deep Neural Networks are often used to perform automatic feature learning, nonlinear mapping of input into output, e.g., classification and regression. Data representations based on neural networks are used for feature learning and nonlinear mapping of inputs and outputs. As deep learning gets more accurate, it can detect false negatives more effectively and operates based on the minimal test. In the article, we will discuss <b>deep learning for test automation</b>.\n",
      "\n",
      "When testing most of the time, test automation is not accurate. Because of which, a lot of deep learning is being implemented. Deep learning is an efficient way to reduce false negatives by performing quantitative labels for testing at a large scale, beyond the possibility of human experts. Companies should figure out how they can use deep learning in test design and execution for achieving the best results.\n",
      "\n",
      "Deep Learning improves the reliability and accuracy of test automation by approaching some of its difficulties. Deep learning is an advanced form of machine learning where algorithms are trained to recognize patterns inhuge data sets rather than relying on rules or being programmed with hard coded logic.\n",
      "\n",
      "Testing is an ever-\n"
     ]
    }
   ],
   "source": [
    "# Prompt for generation\n",
    "prompt = \"Deep Learning is\"\n",
    "\n",
    "res = deployed_model.predict(instances=[\n",
    "  {\"inputs\": prompt, \n",
    "   \"parameters\": {\"max_new_tokens\": 256, \"do_sample\": True, \"top_p\": 0.95, \"temparature\": 1.0 }} # Generation arguments\n",
    "  ]\n",
    ")\n",
    "print(prompt + res.predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up Resources\n",
    "\n",
    "To avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorial, you can delete the resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n",
      "Undeploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/8199922424465063936/operations/1302728898739437568\n",
      "Endpoint model undeployed. Resource name: projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n",
      "Deleting Endpoint : projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n",
      "Delete Endpoint  backing LRO: projects/755607090520/locations/us-central1/operations/994232324264558592\n",
      "Endpoint deleted. . Resource name: projects/755607090520/locations/us-central1/endpoints/8199922424465063936\n",
      "Deleting Model : projects/755607090520/locations/us-central1/models/8539037099238621184\n",
      "Delete Model  backing LRO: projects/755607090520/locations/us-central1/models/8539037099238621184/operations/4205298858579722240\n",
      "Model deleted. . Resource name: projects/755607090520/locations/us-central1/models/8539037099238621184\n"
     ]
    }
   ],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
