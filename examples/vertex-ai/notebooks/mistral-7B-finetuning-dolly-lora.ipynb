{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7546552d",
   "metadata": {},
   "source": [
    "# Finetune Mistral-7B on Vertex AI\n",
    "\n",
    "[Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) is a large language model (LLM) developed by `Mistral AI` and is an instruct fine-tuned version of [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1).\n",
    "\n",
    "In this tutorial you will learn how to finetune [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) on Vertex AI. \n",
    "\n",
    "\n",
    "What you'll learn in this tutorial:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Fine-tune Mistral-7b using `trl` and `SFTTrainer`](#3-fine-tune-mistral-7b-using-trl-and-sfttrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc7977",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "\n",
    "In this example, we will use the Vertex AI Workbench instance with A100 and the [Hugging Face Deep Learning Containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face). The Hugging Face PyTorch DLC comes with all important libraries, like Transformers, Datasets, PEFT, TRL and other packages pre-installed this makes it super easy to get started, since there is no need for environment management. You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "\n",
    "**ToDo**: Add info on how to spin-up a workbench instance or small intro about Vertex AI Workbench Instance.\n",
    "\n",
    "**ToDo**: Update the link for the image once, GPU containers are released. \n",
    "\n",
    "\n",
    "Once the instance is up and running, we can access a Jupyter environment, which we can use for preparing our dataset and launching the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a025a",
   "metadata": {},
   "source": [
    "## 2. Load and prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6b6e2",
   "metadata": {},
   "source": [
    "We will use [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records on categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\\n\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "To load and preprocess the `Dolly` dataset, we use the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3587a9f",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6529f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = (\n",
    "        f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    )\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join(\n",
    "        [i for i in [instruction, context, response] if i is not None]\n",
    "    )\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f911f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.20k/8.20k [00:00<00:00, 24.1MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.1M/13.1M [00:00<00:00, 16.9MB/s]\n",
      "Generating train split: 15011 examples [00:00, 262035.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bd50b",
   "metadata": {},
   "source": [
    "Before applying formatting on our entire dataset, lets test our formatting function on a random example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5381d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Who is the antagonist on The X-Files?', 'context': '', 'response': 'There are many antagonists on The X-Files, but the most long-running individual antagonist is The Cigarette Smoking Man (CSM) also known as C.G.B. Spender. He was primarily responsible for orchestrating conspiracies. He was a member of \"The Syndicate,\" which was a mysterious shadow government group that covered up the existence of extraterrestrial life.', 'category': 'classification', 'text': '### Instruction\\nWho is the antagonist on The X-Files?\\n\\n### Answer\\nThere are many antagonists on The X-Files, but the most long-running individual antagonist is The Cigarette Smoking Man (CSM) also known as C.G.B. Spender. He was primarily responsible for orchestrating conspiracies. He was a member of \"The Syndicate,\" which was a mysterious shadow government group that covered up the existence of extraterrestrial life.'}\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(raw_dataset[randrange(len(raw_dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e0c0b",
   "metadata": {},
   "source": [
    "We can see that the dataset was properly formatted and everything has been appended into one field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cd630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply prompt template\n",
    "format_dataset = raw_dataset.map(\n",
    "    format_dolly, remove_columns=list(raw_dataset.features)\n",
    ")\n",
    "\n",
    "# select only 2500 examples for faster training\n",
    "format_dataset = format_dataset.shuffle(seed=42).select(range(2500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4412849",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Mistral-7b using `trl` and `SFTTrainer`\n",
    "\n",
    "We will use the [SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer) from  ðŸ¤— `trl` to fine-tune our model. The `SFTTrainer`  is built on top of the ðŸ¤— Transformers `Trainer` and inherits all the core functionalities like logging, evaluation, and checkpointing, but offers additional enhancements like:\n",
    "\n",
    "- Packing datasets for more efficient training\n",
    "- PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
    "- Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    "\n",
    "You can read about it in the [trl docs](https://huggingface.co/docs/trl/en/sft_trainer)\n",
    "\n",
    "\n",
    "As, we all know LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. Therefore, we  are going to use [QLoRA](https://arxiv.org/abs/2106.09685), a technqiue technique to reduce the memory footprint of LLMs during finetuning, without sacrificing performance. How it works: \n",
    "\n",
    "- Quantize the pretrained model to 4 bits and freezing it.\n",
    "- Attach small, trainable adapter layers. (LoRA)\n",
    "- Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "To further enhance training efficiency, we'll incorporate a recently introduced, high-performance attention mechanism `Flash Attention 2` alongside `QLoRA`. It is nicely integrated with Transformers. It is up to 3x faster than the standard attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e69f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-25 20:41:25,549] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39524b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" # Hugging Face model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117a1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     #  quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\",             # use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_use_double_quant=True,        # use a nested quantization scheme to quantize the already quantized weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    ")                                          # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e479ea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:54<00:00, 18.11s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\",\n",
    "                                             attn_implementation=\"flash_attention_2\"  # use flash-attention-2 for faster training\n",
    "                                            )\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e074ec",
   "metadata": {},
   "source": [
    "For using QLoRA with SFTTrainer, we need to create our LoraConfig and pass it as an argument to the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aa3c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a41a5",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc130db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"output\",               # directory to save trained model\n",
    "    num_train_epochs = 1,                # number of training epochs\n",
    "    learning_rate = 2e-4,                # learning rate for training\n",
    "    optim=\"paged_adamw_8bit\",            # optimizer for training\n",
    "    per_device_train_batch_size = 1,     # batch size per device during training\n",
    "    gradient_accumulation_steps = 4,     # Number of steps to accumulate gradients before updating the model\n",
    "    logging_steps = 10,                   # log every 10 steps\n",
    "    bf16 = True                          # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    "                                         # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)                                       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e21a3755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [00:00<00:00, 7781.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Initialize the trl SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = format_dataset,\n",
    "    dataset_text_field = \"text\", # field that contains the text in the dataset\n",
    "    args = training_args,\n",
    "    peft_config = peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a20f6ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 12:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.424600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.607500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
