{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7546552d",
   "metadata": {},
   "source": [
    "# Finetune Mistral-7B on Vertex AI\n",
    "\n",
    "[Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) is a large language model (LLM) developed by `Mistral AI` and is an instruct fine-tuned version of [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1).\n",
    "\n",
    "In this tutorial you will learn how to finetune [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) on Vertex AI. \n",
    "\n",
    "\n",
    "What you'll learn in this tutorial:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Fine-tune Mistral-7b using `trl` and `SFTTrainer`](#3-fine-tune-mistral-7b-using-trl-and-sfttrainer)\n",
    "4. [Inference with Fine-tuned Model](#4-inference-with-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc7977",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "\n",
    "In this example, we will use the Vertex AI Workbench instance with A100 and the [Hugging Face Deep Learning Containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face). The Hugging Face PyTorch DLC comes with all important libraries, like Transformers, Datasets, PEFT, TRL and other packages pre-installed this makes it super easy to get started, since there is no need for environment management. You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "\n",
    "**ToDo**: Add info on how to spin-up a workbench instance or small intro about Vertex AI Workbench Instance.\n",
    "\n",
    "**ToDo**: Update the link for the image once, GPU containers are released. \n",
    "\n",
    "\n",
    "Once the instance is up and running, we can access a Jupyter environment, which we can use for preparing our dataset and launching the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a025a",
   "metadata": {},
   "source": [
    "## 2. Load and prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6b6e2",
   "metadata": {},
   "source": [
    "We will use [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records on categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\\n\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "To load and preprocess the `Dolly` dataset, we use the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3587a9f",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = (\n",
    "        f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    )\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join(\n",
    "        [i for i in [instruction, context, response] if i is not None]\n",
    "    )\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bd50b",
   "metadata": {},
   "source": [
    "Before applying formatting on our entire dataset, lets test our formatting function on a random example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(raw_dataset[randrange(len(raw_dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e0c0b",
   "metadata": {},
   "source": [
    "We can see that the dataset was properly formatted and everything has been appended into one field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply prompt template\n",
    "format_dataset = raw_dataset.map(\n",
    "    format_dolly, remove_columns=list(raw_dataset.features)\n",
    ")\n",
    "\n",
    "# select only 2500 examples for faster training\n",
    "format_dataset = format_dataset.shuffle(seed=42).select(range(2500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4412849",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Mistral-7b using `trl` and `SFTTrainer`\n",
    "\n",
    "We will use the [SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer) from  ðŸ¤— `trl` to fine-tune our model. The `SFTTrainer`  is built on top of the ðŸ¤— Transformers `Trainer` and inherits all the core functionalities like logging, evaluation, and checkpointing, but offers additional enhancements like:\n",
    "\n",
    "- Packing datasets for more efficient training\n",
    "- PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
    "- Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    "\n",
    "You can read about it in the [trl docs](https://huggingface.co/docs/trl/en/sft_trainer)\n",
    "\n",
    "\n",
    "As, we all know LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. Therefore, we  are going to use [QLoRA](https://arxiv.org/abs/2106.09685), a technqiue technique to reduce the memory footprint of LLMs during finetuning, without sacrificing performance. How it works: \n",
    "\n",
    "- Quantize the pretrained model to 4 bits and freezing it.\n",
    "- Attach small, trainable adapter layers. (LoRA)\n",
    "- Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "To further enhance training efficiency, we'll incorporate a recently introduced, high-performance attention mechanism `Flash Attention 2` alongside `QLoRA`. It is nicely integrated with Transformers. It is up to 3x faster than the standard attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39524b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" # Hugging Face model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     #  quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\",             # use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_use_double_quant=True,        # use a nested quantization scheme to quantize the already quantized weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    ")                                          # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\",\n",
    "                                             attn_implementation=\"flash_attention_2\"  # use flash-attention-2 for faster training\n",
    "                                            )\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e074ec",
   "metadata": {},
   "source": [
    "For using QLoRA with SFTTrainer, we need to create our LoraConfig and pass it as an argument to the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a41a5",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc130db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"output\",               # directory to save trained model\n",
    "    num_train_epochs = 1,                # number of training epochs\n",
    "    learning_rate = 2e-4,                # learning rate for training\n",
    "    max_grad_norm = 0.3,                 # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio = 0.03,                 # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type = \"constant\",      # use constant learning rate scheduler\n",
    "    optim=\"paged_adamw_8bit\",            # optimizer for training\n",
    "    per_device_train_batch_size = 1,     # batch size per device during training\n",
    "    gradient_accumulation_steps = 4,     # Number of steps to accumulate gradients before updating the model\n",
    "    logging_steps = 100,                 # log every 100 steps\n",
    "    bf16 = True                          # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    "                                         # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)                                       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the trl SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = format_dataset,\n",
    "    dataset_text_field = \"text\", # field that contains the text in the dataset\n",
    "    args = training_args,\n",
    "    peft_config = peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9c5f3",
   "metadata": {},
   "source": [
    "## 4. Inference with Fine-tuned Model\n",
    "\n",
    "Once the fine-tuning is done, we want to run inference on the fine-tuned model. We utilize some prompts from the original dataset and see how does the text generation using the fine-tuned model looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "## Load the adapted model\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(model, \"output\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfe738",
   "metadata": {},
   "source": [
    "Select some prompts for text generation and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Why can camels survive for long without water?\",\n",
    "    \"Are the following items candy bars or gum: trident, Twix, hubba bubba, snickers, three musketeers, and wrigleys.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49178dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = f\"### Instruction\\n {prompt}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.autocast(device):\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb38f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
