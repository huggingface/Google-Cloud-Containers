{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Embedding Models with TEI on Vertex AI\n",
    "\n",
    "This tutorial demonstrates how to deploy any Embedding model from Hugging Face to Vertex AI using Text Embediing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "Before we can install the packages make sure you have the cli installed: https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-storage \"google-auth>=2.23.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vertex AI and SDK\n",
    "\n",
    "Login to gcloud and set your project id.\n",
    "\n",
    "```bash\n",
    "gcloud auth login \n",
    "gcloud auth application-default login\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup SDK with your project id\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"huggingface-ml\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "BUCKET_URI = f\"gs://vertexai-{PROJECT_ID}-tgi\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID} --quiet\n",
    "# Set the region\n",
    "! gcloud config set ai/region {REGION} --quiet\n",
    "# create the bucket if it doesn't exist\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set of constants will be used to create names and display names of Vertex AI Prediction resources like models, endpoints, and model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model names and version\n",
    "MODEL_NAME = \"BGE-Large\" # @param {type:\"string\"}\n",
    "MODEL_VERSION = \"v01\" # @param {type: \"string\"}\n",
    "MODEL_DISPLAY_NAME = f\"TEI-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = f\"endpoint-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "\n",
    "# https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference\n",
    "DOCKER_ARTIFACT_REPO = \"custom-tei-example\" # @param {type:\"string\"}\n",
    "BASE_TGI_IMAGE = \"ghcr.io/huggingface/text-embedding-inference:latest\" # @param {type:\"string\"}\n",
    "SERVING_CONTAINER_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_ARTIFACT_REPO}/base-tei-image:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy model to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/755607090520/locations/us-central1/models/3427166748661514240/operations/8775985187918970880\n",
      "Model created. Resource name: projects/755607090520/locations/us-central1/models/3427166748661514240@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/755607090520/locations/us-central1/models/3427166748661514240@1')\n",
      "TGI-Gemma-7b-v01\n",
      "projects/755607090520/locations/us-central1/models/3427166748661514240\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"BAAI/bge-large-en-v1.5\",\n",
    "        },\n",
    "    serving_container_ports=[80],\n",
    ")\n",
    "\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment will take ~20-25 minutes. You can check the status of the deployment in the cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/755607090520/locations/us-central1/endpoints/3306444769978220544/operations/4289274059151114240\n",
      "Endpoint created. Resource name: projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/755607090520/locations/us-central1/endpoints/3306444769978220544')\n",
      "Deploying model to Endpoint : projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n",
      "Deploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/3306444769978220544/operations/4928785206237724672\n",
      "Endpoint model deployed. Resource name: projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n"
     ]
    }
   ],
   "source": [
    "machine_type = 'g2-standard-4' # L4 GPUs\n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_NAME,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    "    traffic_percentage=100,\n",
    "    min_replica_count=1,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning is attracting much attention recently. It is a chance that it will be incorporated in a navigation system. We are concerned that already deep learning is included in things like smart phones. Ask about various views to give an explanation in natural language comprehensible to the user. It is envisioned that the map creation system is able to collect information on the site using cameras from vehicles.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Deep Learning is\"\n",
    "\n",
    "res = deployed_model.predict(instances=[\n",
    "  {\"inputs\": prompt} ]\n",
    ")\n",
    "print(prompt + res.predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n",
      "Undeploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/3306444769978220544/operations/2260402427020705792\n",
      "Endpoint model undeployed. Resource name: projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n",
      "Deleting Endpoint : projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n",
      "Delete Endpoint  backing LRO: projects/755607090520/locations/us-central1/operations/4050583278900477952\n",
      "Endpoint deleted. . Resource name: projects/755607090520/locations/us-central1/endpoints/3306444769978220544\n",
      "Deleting Model : projects/755607090520/locations/us-central1/models/3427166748661514240\n",
      "Delete Model  backing LRO: projects/755607090520/locations/us-central1/models/3427166748661514240/operations/6872088445448093696\n",
      "Model deleted. . Resource name: projects/755607090520/locations/us-central1/models/3427166748661514240\n"
     ]
    }
   ],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
