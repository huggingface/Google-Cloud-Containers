{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy LLama-2 7B on Vertex AI and interact it through a Gradio UI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-llama-2-on-vertex-ai.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "[LLama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) is a a Llama 2 model with 7B parameters fine-tuned for chat instructions. You can find more information about it on the blogpost [Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2).\n",
    "\n",
    "In this tutorial you will learn how to deploy [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on Vertex AI Endpoints. We are going to use the Hugging Face [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) container which is a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "What you'll learn in this blog:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Configure gcloud CLI](#2-configure-gcloud-cli)\n",
    "3. [Initialize Vertex AI SDK](#3-initialize-vertex-ai-sdk)\n",
    "4. [Deploy Llama-2-7B on Vertex AI](#4-deploy-llama-2-7b-on-vertex-ai)\n",
    "5. [Run Inference with deployed Model](#5-run-inference-with-deployed-model)\n",
    "6. [Create Chatbot UI using Gradio](#6-create-chatbot-ui-using-gradio)\n",
    "7. [Cleaning Up Resources](#7-cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `Vertex AI` python SDK to deploy [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on Vertex AI. You need to have a GCP project and an account with the necessary permissions to create resources in the project. \n",
    "\n",
    "Before we can install the packages we need to install the `gcloud CLI`. Instructions can be found here: https://cloud.google.com/sdk/docs/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages for the notebook\n",
    "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-storage \"google-auth>=2.23.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure gcloud CLI\n",
    "\n",
    "We need to authenticate with Google Cloud SDK to use the Vertex AI services. Run the following command to authenticate with Google Cloud SDK.\n",
    "\n",
    "```bash\n",
    "gcloud auth login \n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gcp-partnership-412108\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "BUCKET_URI = f\"gs://vertexai-{PROJECT_ID}-tgi\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID} --quiet\n",
    "# Set the region\n",
    "! gcloud config set ai/region {REGION} --quiet\n",
    "# create the bucket if it doesn't exist\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Vertex AI SDK \n",
    "\n",
    "The following set of constants will be used to create names and display names of Vertex AI Prediction resources like models, endpoints, and model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model names and version\n",
    "MODEL_NAME = \"Llama-2-7b\" # @param {type:\"string\"}\n",
    "MODEL_VERSION = \"v01\" # @param {type: \"string\"}\n",
    "MODEL_DISPLAY_NAME = f\"TGI-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = f\"endpoint-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "\n",
    "# Set the TGI serving container image uri, selected from https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face\n",
    "SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.1-4.ubuntu2204.py310\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Llama-2-7B on Vertex AI\n",
    "\n",
    "To deploy [Llama-2-7B](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on Vertex AI, we first need to first upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"meta-llama/Llama-2-7b-chat-hf\", # Hugging Face model ID\n",
    "        \"NUM_SHARD\": \"1\", \n",
    "        \"MAX_INPUT_LENGTH\": \"512\", \n",
    "        \"MAX_TOTAL_TOKENS\": \"1024\", \n",
    "        \"HUGGING_FACE_HUB_TOKEN\": \"\", # Replace with your Hugging Face Hub token\n",
    "        },\n",
    "    serving_container_ports=[80],\n",
    ")\n",
    "\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is uploaded, now we can deploy it on Vertex AI Endpoints. \n",
    "First, we need to create an endpoint and then deploy the model to the endpoint. Here you also need to choose the hardware configuration for the deployment. \n",
    "\n",
    "The deployment will take ~20-25 minutes. You can check the status of the deployment in the Google cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_type = 'g2-standard-4' # L4 GPUs\n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_NAME, # The display name of the deployed model\n",
    "    machine_type=machine_type,              # type of machine, read more here: https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "    accelerator_type=\"NVIDIA_L4\",           # Hardware accelerator \n",
    "    accelerator_count=1,                    # Number of accelerators to attach to a worker replica.\n",
    "    min_replica_count=1,                    # The minimum number of machine replicas this deployed model will be always deployed on.\n",
    "    sync=True,                              # Whether to execute this method synchronously.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Inference with deployed Model\n",
    "Awesome! We have successfully deployed the Llama-2-7B model on Vertex AI. Now let's run inference on our endpoint. We will use the `predict` method of the `Endpoint` class to run inference on the deployed model. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [TGI documentation](https://huggingface.github.io/text-generation-inference/) defined under the `GenerateParameters` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "                                          token=\"Your Hugging Face Hub token here\",)\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"What is Hugging Face?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "res = deployed_model.predict(instances=[\n",
    "  {\"inputs\": inputs, \n",
    "   \"parameters\": {\"max_new_tokens\": 256, \"do_sample\": True, \"top_p\": 0.7, \"temparature\": 1.0 }} # Generation arguments\n",
    "  ]\n",
    ")\n",
    "print(res.predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Chatbot UI using Gradio\n",
    "\n",
    "We can create a simple chatbot UI using [Gradio](https://gradio.app/) to interact with our deployed model on Vertex AI Endpoints. Gradio is an open-source Python library that allows you to create UIs for your machine learning models with a few lines of code. You can find more information about Gradio in the [documentation](https://gradio.app/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gradio for interactive UI\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 256, \n",
    "    \"do_sample\": True, \n",
    "    \"top_p\": 0.7, \n",
    "    \"temparature\": 1.0 \n",
    "  }\n",
    "\n",
    "MAX_INPUT_TOKEN_LENGTH = 512 # Set also in the serving container\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Chat with Llama-2-7b!\")\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot()\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                message = gr.Textbox(label=\"Chat Message Box\", placeholder=\"Chat Message Box\", show_label=False)\n",
    "            with gr.Column():\n",
    "                with gr.Row():\n",
    "                    submit = gr.Button(\"Submit\")\n",
    "                    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        conversation = []\n",
    "        for user, assistant in chat_history:\n",
    "            conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "        conversation.append({\"role\": \"user\", \"content\": message})\n",
    "        inputs = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "        # send request to endpoint\n",
    "        llm_response = deployed_model.predict(instances=[{\"inputs\": inputs, \n",
    "                                               \"parameters\": parameters}])\n",
    "        parsed_response = llm_response.predictions[0]\n",
    "        chat_history.append((message, parsed_response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    submit.click(respond, [message, chatbot], [message, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleaning Up Resources\n",
    "\n",
    "To avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorial, you can delete the resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
