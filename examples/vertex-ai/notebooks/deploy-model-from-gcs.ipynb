{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Llama 7B on Vertex AI \n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/llm_streaming_prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official//prediction/llm_streaming_prediction.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official//prediction/llm_streaming_prediction.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "## Resources used \n",
    "* https://cloud.google.com/vertex-ai/docs/predictions/use-tpu\n",
    "* https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/pytorch_image_classification_with_prebuilt_serving_containers.ipynb\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/llm_streaming_prediction.ipynb\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/get_started_with_nvidia_triton_serving.ipynb\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to deploy Llama to Vertex AI using Hugging Face Text Generation Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "Before we can install the packages make sure you have the cli installed: https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-storage \"google-auth>=2.23.3\"\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vertex AI and SDK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated.\n",
    "\n",
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=6fajmwQMFZ0ouvoHuQXEsle3oxNHu1&prompt=consent&access_type=offline&code_challenge=QkTl4yDx9R0W-2EY2-VqcdVQayB9gIB1yzEnja_rEwI&code_challenge_method=S256\n",
      "\n",
      "Enter authorization code: ^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n",
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=YmqcmaWYR4o2gfG6xtb3ac3WSXojzA&prompt=consent&access_type=offline&code_challenge=8QHsJC8MuNKeQAKeCCbpaIZboomCWnsvhIkLGJGVbIs&code_challenge_method=S256\n",
      "\n",
      "Enter authorization code: "
     ]
    }
   ],
   "source": [
    "! gcloud auth login \n",
    "! gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup SDK with your project id\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "PROJECT_ID = \"gcp-partnership-412108\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "# # Set the project id\n",
    "# ! gcloud config set project {PROJECT_ID} --quiet\n",
    "# # Set the region\n",
    "# ! gcloud config set ai/region {REGION} --quiet\n",
    "# # create the bucket if it doesn't exist\n",
    "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "REPOSITORY_ID=\"meta-llama/Meta-Llama-3-8B-Instruct\"  # The repository ID on HuggingFace\n",
    "LOCAL_DIR=\"tmp/llama3\"  # The directory where models will be downloaded\n",
    "GCS_BUCKET=\"gs://hf-gcp-models-deployments-test-3451451/Meta-Llama-3-8B-Instruct/\"  # The Google Cloud Storage bucket to upload models to\n",
    "\n",
    "# Download models from HuggingFace, excluding certain file types\n",
    "mkdir -p $LOCAL_DIR\n",
    "huggingface-cli download $REPOSITORY_ID --exclude \"*.bin\" \"*.pth\" \"*.gguf\" --local-dir $LOCAL_DIR\n",
    "\n",
    "# Upload the downloaded models to Google Cloud Storage\n",
    "gsutil -m cp -r $LOCAL_DIR $GCS_BUCKET\n",
    "\n",
    "# Clean up local directory after upload\n",
    "rm -rf $LOCAL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy model to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/755607090520/locations/us-central1/models/711059667240878080/operations/7358806508936626176\n",
      "Model created. Resource name: projects/755607090520/locations/us-central1/models/711059667240878080@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/755607090520/locations/us-central1/models/711059667240878080@1')\n",
      "llama3\n",
      "projects/755607090520/locations/us-central1/models/711059667240878080\n"
     ]
    }
   ],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = \"us-central1-docker.pkg.dev/gcp-partnership-412108/base-tgi-image/base-tgi-image\"\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"llama3\",\n",
    "    artifact_uri=\"gs://hf-gcp-models-deployments-test-3451451/Meta-Llama-3-8B-Instruct/\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"NUM_SHARD\": \"1\",\n",
    "        \"MAX_INPUT_LENGTH\": \"1512\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
    "        },\n",
    ")\n",
    "\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/755607090520/locations/us-central1/endpoints/6878568932622467072/operations/502076076265046016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created. Resource name: projects/755607090520/locations/us-central1/endpoints/6878568932622467072\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/755607090520/locations/us-central1/endpoints/6878568932622467072')\n",
      "Deploying model to Endpoint : projects/755607090520/locations/us-central1/endpoints/6878568932622467072\n",
      "Deploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/6878568932622467072/operations/7775389474468397056\n",
      "Endpoint model deployed. Resource name: projects/755607090520/locations/us-central1/endpoints/6878568932622467072\n"
     ]
    }
   ],
   "source": [
    "machine_type = 'g2-standard-4' # L4 GPUs\n",
    "endpoint = aiplatform.Endpoint.create(display_name=\"llama3\")\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=\"llama3\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    "    traffic_percentage=100,\n",
    "    min_replica_count=1,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "Google Cloud is a suite of cloud computing services offered by Google. It provides a range of products and services that allow individuals, businesses, and organizations to build, deploy, and manage applications and infrastructure on the cloud. Here are some of the key features and services offered by Google Cloud:\n",
      "\n",
      "1. **Compute Services**: Google Cloud provides a range of compute services, including Google Compute Engine, Google Kubernetes Engine, and Cloud Functions, which allow users to run and manage virtual machines, containers, and serverless functions.\n",
      "2. **Storage Services**: Google Cloud offers a range of storage services, including Google Cloud Storage, Cloud SQL, and Cloud Datastore, which provide scalable and secure storage for data and applications.\n",
      "3. **Big Data and Analytics**: Google Cloud provides a range of big data and analytics services, including Google BigQuery, Google Cloud Dataproc, and Google Cloud Dataflow, which allow users to process and analyze large datasets.\n",
      "4. **Machine Learning and AI**: Google Cloud offers a range of machine learning and AI services, including Google Cloud AI Platform, Google Cloud AutoML, and Google Cloud Vision API, which enable users to build and deploy machine learning models and AI-powered applications.\n",
      "5. **Security and Identity**: Google Cloud provides a range\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"What is Google Cloud?\"},\n",
    "]\n",
    "\n",
    "res = deployed_model.predict(instances=[\n",
    "  {\"inputs\": tok.apply_chat_template(messages,tokenize=False), \n",
    "   \"parameters\": {\"max_new_tokens\": 256, \"do_sample\": True, \"top_p\": 0.7, \"temparature\": 1.0 }}\n",
    "  ]\n",
    ")\n",
    "print(res.predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/1049843053967/locations/us-central1/endpoints/3364094363645771776\n",
      "Undeploy Endpoint model backing LRO: projects/1049843053967/locations/us-central1/endpoints/3364094363645771776/operations/2355565055325503488\n",
      "Endpoint model undeployed. Resource name: projects/1049843053967/locations/us-central1/endpoints/3364094363645771776\n",
      "Deleting Endpoint : projects/1049843053967/locations/us-central1/endpoints/3364094363645771776\n",
      "Delete Endpoint  backing LRO: projects/1049843053967/locations/us-central1/operations/4209922201895305216\n",
      "Endpoint deleted. . Resource name: projects/1049843053967/locations/us-central1/endpoints/3364094363645771776\n",
      "Deleting Model : projects/1049843053967/locations/us-central1/models/7037188878091943936\n",
      "Delete Model  backing LRO: projects/1049843053967/locations/us-central1/models/7037188878091943936/operations/2782281120018857984\n",
      "Model deleted. . Resource name: projects/1049843053967/locations/us-central1/models/7037188878091943936\n"
     ]
    }
   ],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
