{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c432cf-dc16-4bd8-89bd-7c1c0eb58d37",
   "metadata": {},
   "source": [
    "<!-- ---\n",
    "title: Deploy Meta Llama 3.1 405B with TGI DLC on Vertex AI\n",
    "type: inference\n",
    "--- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7faed-c34a-4f01-84ec-eefbfb65506d",
   "metadata": {},
   "source": [
    "# Deploy Meta Llama 3.1 405B with TGI DLC on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018871c-4aa8-43b2-a91a-e00a0d7eecea",
   "metadata": {},
   "source": [
    "[Meta Llama 3.1](https://huggingface.co/blog/llama31) is the latest open LLM from Meta, a follow up iteration of Llama 3, released in July 2024. Meta Llama 3.1 comes in three sizes: 8B for efficient deployment and development on consumer-size GPU, 70B for large-scale AI native applications, and 405B for synthetic data, LLM as a Judge or distillation; among other use cases. Amongst Meta Llama 3.1 new features, the ones to highlight are: a large context length of 128K tokens (vs original 8K), multilingual capabilities, tool usage capabilities, and a more permissive license.\n",
    "\n",
    "This example showcases how to deploy [`meta-llama/Meta-Llama-3.1-405B-Instruct-FP8`](https://hf.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8) on Vertex AI with an A3 accelerator-optimized instance with 8 NVIDIA H100s via the Hugging Face purpose-built Deep Learning Container (DLC) for Text Generation Inference (TGI) on Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb745ca-5e31-48e3-8e66-0e352327a9f9",
   "metadata": {},
   "source": [
    "![`meta-llama/Meta-Llama-3.1-405B-Instruct-FP8` in the Hugging Face Hub](./assets/model-in-hf-hub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8fdb1-12d3-4463-bf24-4f17125d7860",
   "metadata": {},
   "source": [
    "## Setup / Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c001a-408a-45fc-b3c8-076d9f7247da",
   "metadata": {},
   "source": [
    "First, you need to install `gcloud` in your local machine, which is the command-line tool for Google Cloud, following the instructions at [Cloud SDK Documentation - Install the gcloud CLI](https://cloud.google.com/sdk/docs/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0b2ec-2ba0-4567-b028-1bf060a393ce",
   "metadata": {},
   "source": [
    "Then, you also need to install the `google-cloud-aiplatform` Python SDK, required to programmatically create the Vertex AI model, register it, acreate the endpoint, and deploy it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2caea-90e5-4b10-8aff-4a0d3cb378db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18752459-400e-47c4-893a-66e181172084",
   "metadata": {},
   "source": [
    "Optionally, to ease the usage of the commands within this tutorial, you need to set the following environment variables for GCP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d396d4a-12fb-4516-9866-10e8f8cfb005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROJECT_ID=your-project-id\n",
    "%env LOCATION=your-location\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f8a28-47b4-4988-af95-f9c817b9fe73",
   "metadata": {},
   "source": [
    "Then you need to login into your GCP account and set the project ID to the one you want to use to register and deploy the models on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b93d77-645c-4803-a301-654a1dd7d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud auth application-default login  # For local development\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70872ace-2d41-4c98-b1c2-5faa4f8b933b",
   "metadata": {},
   "source": [
    "Once you are logged in, you need to enable the necessary service APIs in GCP, such as the Vertex AI API, the Compute Engine API, and Google Container Registry related APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417c086-b15e-42dc-b1a4-1f56bacebfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "!gcloud services enable container.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com\n",
    "!gcloud services enable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7c90d-7140-41f8-9f7f-0e832c760cd1",
   "metadata": {},
   "source": [
    "Once everything is set up, you can already initialize the Vertex AI session via the [`google-cloud-aiplatform`](https://github.com/googleapis/python-aiplatform) Python SDK as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c873a1-8ae1-40c4-9997-9ad00a5535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=os.getenv(\"PROJECT_ID\"),\n",
    "    location=os.getenv(\"LOCATION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47428f04-f10e-4c23-9f4c-a5f91b05e14d",
   "metadata": {},
   "source": [
    "### Quotas on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2e521-728f-4661-91ce-604e56718263",
   "metadata": {},
   "source": [
    "To serve [`meta-llama/Meta-Llama-3.1-405B-Instruct-FP8`](https://hf.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8) you need an instance with at least 400GiB of GPU VRAM that supports the FP8 data-type, and the A3 accelerator-optimized machines on Google Cloud are the machines you would need to use.\n",
    "\n",
    "Even if the A3 accelerator-optimized machines with 8 x NVIDIA H100 80GB GPUs are available within Google Cloud, you will still need to request a custom quota increase in Google Cloud, as those need a specific approval. Note that the A3 accelerator-optimized machines are only available in some zones, so make sure to check the availability of both A3 High or even A3 Mega per zone at [Compute Engine - GPU regions and zones](https://cloud.google.com/compute/docs/gpus/gpu-regions-zones).\n",
    "\n",
    "![A3 availability in Google Cloud](./assets/a3-general-availability.png)\n",
    "\n",
    "In this case, to request a quota increase to use the machine with 8 NVIDIA H100s you will need to increase the following quotas:\n",
    "\n",
    "* `Service: Vertex AI API` and `Name: Custom model serving Nvidia H100 80GB GPUs per region` set to **8**\n",
    "* `Service: Vertex AI API` and `Name: Custom model serving A3 CPUs per region` set to **208**\n",
    "\n",
    "![A3 Quota Request in Google Cloud](./assets/a3-quota-request.png)\n",
    "\n",
    "Read more on how to request a quota increase at [Google Cloud Documentation - View and manage quotas](https://cloud.google.com/docs/quotas/view-manage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c171e7-22f9-4107-ae44-a6c0ba173c01",
   "metadata": {},
   "source": [
    "## Register model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0094009-7f3e-4c2e-ba90-c393db353a7c",
   "metadata": {},
   "source": [
    "Since [`meta-llama/Meta-Llama-3.1-405B-Instruct-FP8`](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8) is a gated model, you need to login into your Hugging Face Hub account, accept the gating requirements, and then generate an access token either with fine-grained read access to the gated model only (recommended), or read-access to your account.\n",
    "\n",
    "Read more about [access tokens for the Hugging Face Hub](https://huggingface.co/docs/hub/en/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d7737-9f7e-43ba-80f8-bbe28d6257c1",
   "metadata": {},
   "source": [
    "To authenticate, you can either use the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Python SDK as shown below (recommended), or just set the environment variable `HF_TOKEN` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9512411-00e4-4cd4-be4e-026dece76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3dda7-c37a-4cbe-9c31-18b51a6fe076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f1274-5535-4907-93bb-8d398e437e86",
   "metadata": {},
   "source": [
    "Then you can already \"upload\" the model i.e. register the model on Vertex AI. It is not an upload per se, since the model will be automatically downloaded from the Hugging Face Hub in the Hugging Face DLC for TGI on startup via the `MODEL_ID` environment variable, so what is uploaded is only the configuration, not the model weights.\n",
    "\n",
    "Before going into the code, let's quickly review the arguments provided to the `upload` method:\n",
    "\n",
    "- **`display_name`** is the name that will be shown in the Vertex AI Model Registry.\n",
    "- **`serving_container_image_uri`** is the location of the Hugging Face DLC for TGI that will be used for serving the model.\n",
    "- **`serving_container_environment_variables`** are the environment variables that will be used during the container runtime, so these are aligned with the environment variables defined by TGI via the [`text-generation-launcher`](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher), which exposes some environment variables such as the following:\n",
    "    - `MODEL_ID` the model ID on the Hugging Face Hub.\n",
    "    - `NUM_SHARD` the number of shards to use i.e. the number of GPUs to use, in this case set to 8 as a node with 8 NVIDIA H100s will be used.\n",
    "    - `HUGGING_FACE_HUB_TOKEN` is the Hugging Face Hub token, required as [`meta-llama/Meta-Llama-3.1-405B-Instruct-FP8`](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8) is a gated model.\n",
    "    - `HF_HUB_ENABLE_HF_TRANSFER` to enable a faster download speed via the [`hf_transfer`](https://github.com/huggingface/hf_transfer) library.\n",
    "\n",
    "For more information on the supported arguments, check [`aiplatform.Model.upload` Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9392297-604f-488d-a9dc-1c2143388a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_token\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"meta-llama--Meta-Llama-3.1-405B-Instruct-FP8\",\n",
    "    serving_container_image_uri=\"\",\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": get_token(),\n",
    "        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "        \"NUM_SHARD\": \"8\",\n",
    "    },\n",
    ")\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4da6b-24fa-403b-9ab1-52ad3a1b3f8a",
   "metadata": {},
   "source": [
    "![Meta Llama 3.1 405B FP8 registered on Vertex AI](./assets/vertex-ai-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecc82c-5b02-4f91-b781-f5a5faaf22f4",
   "metadata": {},
   "source": [
    "## Deploy model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9bada-eee6-43b3-8c06-988d0a43217b",
   "metadata": {},
   "source": [
    "Once Meta Llama 3.1 405B is registered on Vertex AI Model Registry, you can already deploy it on a Vertex AI Endpoint with the Hugging Face DLC for TGI.\n",
    "\n",
    "The `deploy` method will link the previously created endpoint resource with the model that contains the configuration of the serving container, and then, it will deploy the model on Vertex AI in the specified instance.\n",
    "\n",
    "Before going into the code, let's quickly review the arguments provided to the `deploy` method:\n",
    "\n",
    "- **`endpoint`** is the endpoint to deploy the model to, which is optional, and by default will be set to the model display name with the `_endpoint` suffix.\n",
    "- **`machine_type`**, **`accelerator_type`** and **`accelerator_count`** are arguments that define which instance to use, and additionally, the accelerator to use and the number of accelerators, respectively. The `machine_type` and the `accelerator_type` are tied together, so you will need to select an instance that supports the accelerator that you are using and vice-versa. More information about the different instances at [Compute Engine Documentation - GPU machine types](https://cloud.google.com/compute/docs/gpus), and about the `accelerator_type` naming at [Vertex AI Documentation - MachineSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec).\n",
    "\n",
    "For more information on the supported arguments you can check [`aiplatform.Model.deploy` Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe2a86-d333-4802-a55b-9917584929cd",
   "metadata": {},
   "source": [
    "> **Note**: As mentioned before, since Meta Llama 3.1 405B in FP8 takes ~400 GiB of disk space, that means you need at least 400 GiB of GPU VRAM to load the model, and the GPUs within the node need to support the FP8 data type. In this case, an A3 instance with 8 x NVIDIA H100 80GB with a total of ~640 GiB of VRAM will be used to load the model while also leaving some free VRAM for the KV Cache and the CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160c31d-2aba-4222-b166-a2590b17f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = model.deploy(\n",
    "    endpoint=aiplatform.Endpoint.create(display_name=\"Meta-Llama-3.1-405B-FP8-Endpoint\"),\n",
    "    machine_type=\"a3-highgpu-8g\",\n",
    "    accelerator_type=\"NVIDIA_H100_80GB\",\n",
    "    accelerator_count=8,\n",
    "    enable_access_logging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd3890-eeb1-42a5-86f1-471c06194147",
   "metadata": {},
   "source": [
    "> **Disclaimer**: [`meta-llama/Meta-Llama-3.1-405B-Instruct-FP8`](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8) deployment on Vertex AI will take \\~30 minutes to deploy, as it needs to allocate the resources on Google Cloud, and then download the weights from the Hugging Face Hub (\\~10 minutes) and load those for inference in TGI (\\~3 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11256904-be14-42b8-99e8-e035cccd23fd",
   "metadata": {},
   "source": [
    "![Meta Llama 3.1 405B Instruct FP8 deployed on Vertex AI](./assets/vertex-ai-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a3564-95db-4c96-af9f-364664c60bbd",
   "metadata": {},
   "source": [
    "## Online predictions on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cefdb1-91de-4ed9-9410-6a20c6523e01",
   "metadata": {},
   "source": [
    "Finally, you can run the online predictions on Vertex AI using the `predict` method, which will send the requests to the running endpoint in the `/predict` route specified within the container following Vertex AI I/O payload formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bab73-8f35-4175-a41f-268b856d8969",
   "metadata": {},
   "source": [
    "> **Disclaimer**: Manually formatting the data to be compliant with the chat template before sending the request is not required for base models without a chat template, neither from TGI 2.3 onwards, as it will come with support for the OpenAI-compatible endpoint i.e. `/v1/chat/completions` via `MESSAGES_API_ENABLED=1`, which applies the chat template on the server side, so the request can contain the conversation messages unformatted but following OpenAI-specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd118290-16f4-4855-91e7-2aba1dc9b255",
   "metadata": {},
   "source": [
    "As `/generate` is the endpoint that is being exposed through TGI on Vertex AI, you will need to format the messages with the chat template before sending the request to Vertex AI, so you will need to install 🤗`transformers` to use the `apply_chat_template` method from the `PreTrainedTokenizerFast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab0add-cd2c-467f-94f3-5c44024968f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade --quiet transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77455d-52b4-4416-a8a0-78b80cb7aba3",
   "metadata": {},
   "source": [
    "And then apply the chat template to a conversation using the tokenizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00551add-23c8-479c-93b8-bf16d1c44123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import get_token\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
    "    token=get_token(),\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant that responds as a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the Theory of Relativity?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "# <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an assistant that responds as a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's the Theory of Relativity?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b6801-c2ba-41a1-ad32-5a2f28e86653",
   "metadata": {},
   "source": [
    "Which is what you will be sending within the payload to the deployed Vertex AI Endpoint, as well as the generation parameters as in [Consuming Text Generation Inference (TGI) -> Generate](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0d235-4a4e-4695-98e3-42e77a1dd4ed",
   "metadata": {},
   "source": [
    "### Via Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d507c-f763-4a30-bef4-c09157f6a72e",
   "metadata": {},
   "source": [
    "#### Within the same session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bf178-a1ee-40ee-aed5-2011eb8dc43b",
   "metadata": {},
   "source": [
    "If you are willing to run the online prediction within the current session, you can send requests programmatically via the `aiplatform.Endpoint` (returned by the `aiplatform.Model.deploy` method) as in the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde447c-5f44-4c0c-928b-7251736e9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(\n",
    "    instances=[\n",
    "        {\n",
    "            \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an assistant that responds as a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's the Theory of Relativity?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 1.0,\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(output.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de947a-a79d-4c58-a240-0c1aa0491c40",
   "metadata": {},
   "source": [
    "Producing the following `output`:\n",
    "\n",
    "```\n",
    "Prediction(predictions=[\"Yer want ta know about them fancy science things, eh? Alright then, matey, settle yerself down with a pint o' grog and listen close. I be tellin' ye about the Theory o' Relativity, as proposed by that swashbucklin' genius, Albert Einstein.\\n\\nNow, ye see, Einstein said that time and space be connected like the sea and the wind. Ye can't have one without the other, savvy? And he proposed that how ye see time and space depends on how fast ye be movin' and where ye be standin'. That be called relativity, me\"], deployed_model_id='***', metadata=None, model_version_id='1', model_resource_name='projects/***/locations/us-central1/models/***', explanations=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8e648-4c30-4d1a-8466-2d31206f0491",
   "metadata": {},
   "source": [
    "#### From a different session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790fad1-36f5-4aee-aa00-2ec924618fa4",
   "metadata": {},
   "source": [
    "If the Vertex AI Endpoint was deployed in a different session and you want to use it but don't have access to the `deployed_model` variable returned by the `aiplatform.Model.deploy` method as in the previous section; you can also run the following snippet to instantiate the deployed `aiplatform.Endpoint` via its resource name as `projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}`.\n",
    "\n",
    "> Note that you will need to either retrieve the resource name i.e. the `projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}` URL yourself via the Google Cloud Console, or just replace the `ENDPOINT_ID` below that can either be found via the previously instantiated `endpoint` as `endpoint.id` or via the Google Cloud Console under the Online predictions where the endpoint is listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77863e-c46d-4147-ba05-37aa42275fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=os.getenv(\"PROJECT_ID\"), location=os.getenv(\"LOCATION\"))\n",
    "\n",
    "endpoint_display_name = \"Meta-Llama-3.1-405B-FP8-Endpoint\"  # TODO: change to your endpoint display name\n",
    "\n",
    "# Iterates over all the Vertex AI Endpoints within the current project and keeps the first match (if any), otherwise set to None\n",
    "ENDPOINT_ID = next(\n",
    "    (endpoint.name for endpoint in aiplatform.Endpoint.list() \n",
    "     if endpoint.display_name == endpoint_display_name), \n",
    "    None\n",
    ")\n",
    "assert ENDPOINT_ID, (\n",
    "    \"`ENDPOINT_ID` is not set, please make sure that the `endpoint_display_name` is correct at \"\\\n",
    "    f\"https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={os.getenv('PROJECT_ID')}\"\n",
    ")\n",
    "\n",
    "endpoint = aiplatform.Endpoint(f\"projects/{os.getenv('PROJECT_ID')}/locations/{os.getenv('LOCATION')}/endpoints/{ENDPOINT_ID}\")\n",
    "output = endpoint.predict(\n",
    "    instances=[\n",
    "        {\n",
    "            \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an assistant that responds as a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's the Theory of Relativity?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 0.7,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(output.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e0045-22ee-4570-b1ba-dcca7d214911",
   "metadata": {},
   "source": [
    "Producing the following `output`:\n",
    "\n",
    "```\n",
    "Prediction(predictions=[\"Yer lookin' fer a treasure trove o' knowledge about them fancy physics, eh? Alright then, matey, settle yerself down with a pint o' grog and listen close, as I spin ye the yarn o' Einstein's Theory o' Relativity.\\n\\nIt be a tale o' two parts, me hearty: Special Relativity and General Relativity. Now, I know what ye be thinkin': what in blazes be the difference? Well, matey, let me break it down fer ye.\\n\\nSpecial Relativity be the idea that time and space be connected like the sea and the sky.\"], deployed_model_id='***', metadata=None, model_version_id='1', model_resource_name='projects/***/locations/us-central1/models/***', explanations=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cf42d-fc56-4362-8c07-c15cb4488356",
   "metadata": {},
   "source": [
    "### Via the Vertex AI Online Prediction UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739a077-b3ef-4700-937a-9bece4cc165a",
   "metadata": {},
   "source": [
    "Alternatively, for testing purposes you can also use the Vertex AI Online Prediction UI, that provides a field that expects the JSON payload formatted according to the Vertex AI specification (as in the examples above) being:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an assistant that responds as a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's the Theory of Relativity?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"do_sample\": true,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8186aad-41d0-4649-916e-98144f572632",
   "metadata": {},
   "source": [
    "![Meta Llama 3.1 405B Instruct FP8 online prediction on Vertex AI](./assets/vertex-ai-online-prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfdde22-87d5-4e75-8cd0-45e85a0b81fe",
   "metadata": {},
   "source": [
    "## Resource clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55296eb8-f3e8-4417-91e7-39a6bb2d0331",
   "metadata": {},
   "source": [
    "Finally, you can release the resources that you've created as follows, to avoid unnecessary costs:\n",
    "\n",
    "- `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n",
    "- `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.\n",
    "- `model.delete` to delete the model from the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6268d-df0a-4596-8433-dc47289b2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27651cb-2749-419e-a38f-61df86c4ace5",
   "metadata": {},
   "source": [
    "Alternatively, you can also remove those from the Google Cloud Console following the steps:\n",
    "* Go to Vertex AI in Google Cloud\n",
    "* Go to Deploy and use -> Online prediction\n",
    "* Click on the endpoint and then on the deployed model/s to \"Undeploy model from endpoint\"\n",
    "* Then go back to the endpoint list and remove the endpoint\n",
    "* Finally, go to Deploy and use -> Model Registry, and remove the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
