{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553c8548-0a0a-4dc9-ae22-1f9adefedb50",
   "metadata": {},
   "source": [
    "# Finetune Gemma-7B on Vertex AI\n",
    "\n",
    "\n",
    "[Gemma-7b](https://huggingface.co/google/gemma-7b) is a state-of-the-art open model from Google, built from the same research and technology used to create the Gemini models. It is a text-to-text, decoder-only large language model, available in English, with open weights, and is really well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Learn more about it [Welcome Gemma - Googleâ€™s new open LLM](https://huggingface.co/blog/gemma). \n",
    "\n",
    "In this tutorial you will learn how to finetune [google/gemma-7b](https://huggingface.co/google/gemma-7b) on Vertex AI. \n",
    "\n",
    "\n",
    "What you'll learn in this tutorial:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Fine-tune Gemma-7b using `trl` and `SFTTrainer`](#3-fine-tune-gemma-7b-using-trl-and-sfttrainer)\n",
    "4. [Inference with Fine-tuned Model](#4-inference-with-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8cb33c-8042-4110-bc57-9544a59001da",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "\n",
    "In this example, we will use the Vertex AI Workbench instance with A100 and the [Hugging Face Deep Learning Containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face). The Hugging Face PyTorch DLC comes with all important libraries, like Transformers, Datasets, PEFT, TRL and other packages pre-installed this makes it super easy to get started, since there is no need for environment management. You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "\n",
    "**ToDo**: Add info on how to spin-up a workbench instance or small intro about Vertex AI Workbench Instance.\n",
    "\n",
    "**ToDo**: Update the link for the image once, GPU containers are released. \n",
    "\n",
    "\n",
    "Once the instance is up and running, we can access a Jupyter environment, which we can use for preparing our dataset and launching the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9116241-c1a3-4af3-a6cc-baf268beb8a8",
   "metadata": {},
   "source": [
    "## 2. Load the dataset \n",
    "\n",
    "We use the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset, which is a refined part of the [OpenAssistant dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) designed specifically to train versatile chatbots. The dataset contains various questions that require generative outputs.\n",
    "\n",
    "The data is like a question along with its answer. Further, its multi-lingual, i.e., we have questions in English and in Spanish. The dataset contains about 9.85K training instances along with 518 test instances. An example from the dataset:\n",
    "\n",
    "```\n",
    "###Human: Can you write a joke with the following setup? A penguin and a walrus walk into a bar### Assistant: A penguin and a walrus walk into a bar. The bartender looks up and says, \"What is this, some kind of Arctic joke?\" The penguin and walrus just look at each other, confused. Then the walrus shrugs and says, \"I don't know about Arctic jokes, but we sure know how to break the ice!\" The penguin rolls his eyes, but can't help but chuckle.\n",
    "```\n",
    "\n",
    "\n",
    "To load the `timdettmers/openassistant-guanaco` dataset, we use the load_dataset() method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259e5fb-bc76-42d7-a58b-e844b8ec07ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary library for loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the name of the dataset\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "# Load the dataset from the specified name and select the \"train\" split\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Select only 2500 examples for faster training\n",
    "dataset = dataset.shuffle(seed=42).select(range(2500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96db79",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Gemma-7b using `trl` and `SFTTrainer`\n",
    "\n",
    "We will use the [SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer) from  ðŸ¤— `trl` to fine-tune our model. The `SFTTrainer`  is built on top of the ðŸ¤— Transformers `Trainer` and inherits all the core functionalities like logging, evaluation, and checkpointing, but offers additional enhancements like:\n",
    "\n",
    "- Packing datasets for more efficient training\n",
    "- PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
    "- Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    "\n",
    "You can read about it in the [trl docs](https://huggingface.co/docs/trl/en/sft_trainer)\n",
    "\n",
    "\n",
    "As, we all know LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. Therefore, we  are going to use [QLoRA](https://arxiv.org/abs/2106.09685), a technqiue technique to reduce the memory footprint of LLMs during finetuning, without sacrificing performance. How it works: \n",
    "\n",
    "- Quantize the pretrained model to 4 bits and freezing it.\n",
    "- Attach small, trainable adapter layers. (LoRA)\n",
    "- Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "To further enhance training efficiency, we'll incorporate a recently introduced, high-performance attention mechanism `Flash Attention 2` alongside `QLoRA`. It is nicely integrated with Transformers. It is up to 3x faster than the standard attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e5e44-744d-45fb-ab85-26a7ce0d994f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     #  quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\",             # use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_use_double_quant=True,        # use a nested quantization scheme to quantize the already quantized weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    ")                                          # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21985c",
   "metadata": {},
   "source": [
    "In order to use `google/gemma-7b` you will need the Hugging Face Hub Token, so make sure to execute the following:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login # The easiest way to authenticate and it saves the token on your machine. \n",
    "```\n",
    "\n",
    "There are other ways too which can be found in the [docs](https://huggingface.co/docs/huggingface_hub/en/quick-start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bea530-d950-4c52-8109-d82ee79edeec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config = config,\n",
    "                                             attn_implementation = \"flash_attention_2\", # use flash-attention-2 for faster training\n",
    "                                             device_map = \"auto\",\n",
    "                                             torch_dtype=torch.bfloat16\n",
    "                                            ) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"right\"  # to prevent warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ee3ac-d4cf-46e5-a0dc-b36e54f0a6dc",
   "metadata": {},
   "source": [
    "For using QLoRA with SFTTrainer, we need to create our LoraConfig and pass it as an argument to the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028720e-409d-4bd1-9daa-d769797ee8be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"Causal_LM\", \n",
    "    target_modules=\"all-linear\", \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672b756",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a54a0a-0314-4c76-b35f-f694da5ec097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"output\",               # directory to save trained model\n",
    "    num_train_epochs = 2,                # number of training epochs\n",
    "    learning_rate = 2e-4,                # learning rate for training\n",
    "    max_grad_norm = 0.3,                 # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio = 0.03,                 # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type = \"constant\",      # use constant learning rate scheduler\n",
    "    optim = \"paged_adamw_8bit\",          # optimizer for training\n",
    "    per_device_train_batch_size = 1,     # batch size per device during training\n",
    "    gradient_accumulation_steps = 4,     # Number of steps to accumulate gradients before updating the model\n",
    "    logging_steps = 100,                 # log every 100 steps\n",
    "    bf16 = True                          # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    "                                         # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)                                       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the trl SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", # field that contains the text in the dataset\n",
    "    args = training_args,\n",
    "    peft_config = peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b7a60-c8a3-4730-a95d-ea7920e46b47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3bbbe",
   "metadata": {},
   "source": [
    "## 4. Inference with Fine-tuned Model\n",
    "\n",
    "Once the fine-tuning is done, we want to run inference on the fine-tuned model. We utilize some prompts from the original dataset and see how does the text generation using the fine-tuned model looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "## Load the adapted model\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = PeftModel.from_pretrained(model, \"output\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc22cf",
   "metadata": {},
   "source": [
    "Select some prompts for text generation and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"### Human: Explain in layman's terms what does options trading mean?\",\n",
    "    \"### Human: Was kannst Du im Vergleich zu anderen Large Language Models?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.autocast(device):\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de585b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6eb389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
