{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Falcon 7B on Vertex AI \n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-falcon-on-vertex-ai.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "[Falcon-7b-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) is a 7B parameters causal decoder-only model built by TII based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license. You can find more information about other Falcon model in the blog post [The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon).\n",
    "\n",
    "In this tutorial you will learn how to deploy [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) on Vertex AI Endpoints. We are going to use the Hugging Face [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) container which is a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "What you'll learn in this blog:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Configure gcloud CLI](#2-configure-gcloud-cli)\n",
    "3. [Initialize Vertex AI SDK](#3-initialize-vertex-ai-sdk)\n",
    "4. [Deploy Falcon-7B on Vertex AI](#4-deploy-falcon-7b-on-vertex-ai)\n",
    "5. [Run Inference with deployed Model](#5-run-inference-with-deployed-model)\n",
    "6. [Cleaning Up Resources](#6-cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `Vertex AI` python SDK to deploy [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) on Vertex AI. You need to have a GCP project and an account with the necessary permissions to create resources in the project. \n",
    "\n",
    "Before we can install the packages we need to install the `gcloud CLI`. Instructions can be found here: https://cloud.google.com/sdk/docs/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages for the notebook\n",
    "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-storage \"google-auth>=2.23.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure gcloud CLI\n",
    "\n",
    "We need to authenticate with Google Cloud SDK to use the Vertex AI services. Run the following command to authenticate with Google Cloud SDK.\n",
    "\n",
    "```bash\n",
    "gcloud auth login \n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gcp-project-id\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "BUCKET_URI = f\"gs://vertexai-{PROJECT_ID}-tgi\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID} --quiet\n",
    "# Set the region\n",
    "! gcloud config set ai/region {REGION} --quiet\n",
    "# create the bucket if it doesn't exist\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Vertex AI SDK \n",
    "\n",
    "The following set of constants will be used to create names and display names of Vertex AI Prediction resources like models, endpoints, and model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model names and version\n",
    "MODEL_NAME = \"falcon-7b-hf\" # @param {type:\"string\"}\n",
    "MODEL_VERSION = \"v01\" # @param {type: \"string\"}\n",
    "MODEL_DISPLAY_NAME = f\"TGI-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = f\"endpoint-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "\n",
    "# Set the TGI serving container image uri, selected from https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face\n",
    "SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.1-4.ubuntu2204.py310\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"tiiuae/falcon-7b-instruct\", # Hugging Face model ID\n",
    "        \"NUM_SHARD\": \"1\",\n",
    "        \"MAX_INPUT_LENGTH\": \"1512\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
    "        },\n",
    "    \n",
    "    serving_container_ports=[80],\n",
    ")\n",
    "\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is uploaded, now we can deploy it on Vertex AI Endpoints. \n",
    "First, we need to create an endpoint and then deploy the model to the endpoint. Here you also need to choose the hardware configuration for the deployment. \n",
    "\n",
    "The deployment will take ~20-25 minutes. You can check the status of the deployment in the Google cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_type = 'g2-standard-8' # L4 GPUs\n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_NAME, # The display name of the deployed model\n",
    "    machine_type=machine_type,              # type of machine, read more here: https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "    accelerator_type=\"NVIDIA_L4\",           # Hardware accelerator \n",
    "    accelerator_count=1,                    # Number of accelerators to attach to a worker replica.\n",
    "    traffic_percentage=100,                 # Percentage of traffic to send to this model\n",
    "    min_replica_count=1,                    # The minimum number of machine replicas this deployed model will be always deployed on.\n",
    "    sync=True,                              # Whether to execute this method synchronously.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Inference with deployed Model\n",
    "Awesome! We have successfully deployed the Falcon-7B model on Vertex AI. Now let's run inference on our endpoint. We will use the `predict` method of the `Endpoint` class to run inference on the deployed model. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [TGI documentation](https://huggingface.github.io/text-generation-inference/) defined under the `GenerateParameters` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for generation\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Falcon who knows everything about Google Cloud.\n",
    "\n",
    "User: Can you tell me about Google Cloud Vertex AI?\n",
    "Falcon:\"\"\"\n",
    "\n",
    "res = deployed_model.predict(instances=[\n",
    "  {\"inputs\": prompt, \n",
    "   \"parameters\": {\"max_new_tokens\": 128, \n",
    "                  \"do_sample\": True, \n",
    "                  \"top_p\": 0.9, \n",
    "                  \"temparature\": 1.0, \n",
    "                  }} # Generation arguments\n",
    "  ]\n",
    ")\n",
    "print(res.predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up Resources\n",
    "\n",
    "To avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorial, you can delete the resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
