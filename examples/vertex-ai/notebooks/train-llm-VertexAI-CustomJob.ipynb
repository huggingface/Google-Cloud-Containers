{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf1b0d-4fea-4d4a-97d0-779406187055",
   "metadata": {},
   "source": [
    "# Finetuning LLMs with Vertex AI Custom Job on Google Cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab86374-bf99-488c-a6aa-bb0b7ca0ad37",
   "metadata": {},
   "source": [
    "\n",
    "## Setup Development Environment\n",
    "1. A Google Cloud Project.\n",
    "2. [A service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User`, `Storage Object Admin` and `Artifact Registry Reader` roles for finetuning model.\n",
    "3. Make sure you have `Python >=3.8`\n",
    "4. Install the Google Cloud SDK with: `curl https://sdk.cloud.google.com | bash`\n",
    "5. Authenticate your gcloud sdk with \n",
    "\n",
    "```bash\n",
    "gcloud auth login\n",
    "gcloud config set project <your-project-id>\n",
    "gcloud auth application-default login\n",
    "```\n",
    "6. Install the `google-cloud-aiplatform` using\n",
    "```bash\n",
    "pip install google-cloud-aiplatform\n",
    "```\n",
    "7. Make sure you have access to the [Google-Cloud-Containers](https://github.com/huggingface/Google-Cloud-Containers.git) GitHub repository and clone it using:\n",
    "```bash\n",
    "git clone https://github.com/huggingface/Google-Cloud-Containers.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73c01e-d079-49f1-9626-11cd60480e8c",
   "metadata": {},
   "source": [
    "## Build Docker Image and Push it to Artifact Registry\n",
    "\n",
    "The User Account that you used in `Step 5` in `Setup Development Environment` must have `Artifact Registry Writer` role to push the image to the `Artifact Registry`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c926d61c-ea25-4ae6-81f3-bdd3ea1125dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing push-to-gar.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile push-to-gar.sh \n",
    "\n",
    "#!/bin/bash\n",
    "####################################################################\n",
    "# Description: Builds docker image and pushes to Google Artifact-Registry \n",
    "####################################################################\n",
    "\n",
    "REGION=\"us-central1\"\n",
    "DOCKER_ARTIFACT_REPO=\"deep-learning-images\"\n",
    "PROJECT_ID=\"project-id\" # Set it properly\n",
    "FRAMEWORK=\"pytorch\"\n",
    "TYPE=\"training\"\n",
    "ACCELERATOR=\"gpu\"\n",
    "FRAMEWORK_VERSION=\"2.1\"\n",
    "TRANSFORMERS_VERISON=\"4.38.1\"\n",
    "PYTHON_VERSION=\"py310\"\n",
    "\n",
    "SERVING_CONTAINER_IMAGE_URI=\"${REGION}-docker.pkg.dev/${PROJECT_ID}/${DOCKER_ARTIFACT_REPO}/huggingface-${FRAMEWORK}-${TYPE}-${ACCELERATOR}.${FRAMEWORK_VERSION}.transformers.${TRANSFORMERS_VERISON}.${PYTHON_VERSION}:latest\"\n",
    "\n",
    "# Set Google-Cloud Region\n",
    "gcloud config set region \"${REGION}\" --quiet\n",
    "\n",
    "# Enable Artifact-Registry API\n",
    "gcloud services enable artifactregistry.googleapis.com\n",
    "\n",
    "# create a new Docker repository with your region with the description\n",
    "gcloud artifacts repositories create \"${DOCKER_ARTIFACT_REPO}\" \\\n",
    "  --repository-format=docker \\\n",
    "  --location=\"${REGION}\" \\\n",
    "  --description=\"Deep Learning Images\"\n",
    "\n",
    "# verify that your repository was created.\n",
    "gcloud artifacts repositories list \\\n",
    "  --location=\"${REGION}\" \\\n",
    "  --filter=\"name~${DOCKER_ARTIFACT_REPO}\"\n",
    "\n",
    "# configure docker to use your repository    \n",
    "gcloud auth configure-docker \"${REGION}-docker.pkg.dev\"\n",
    "\n",
    "# build and push\n",
    "docker build -t \"${SERVING_CONTAINER_IMAGE_URI}\" -f \"Google-Cloud-Containers/containers/pytorch/training/gpu/2.1/transformers/4.38.1/py310/Dockerfile\" .\n",
    "\n",
    "docker push \"${SERVING_CONTAINER_IMAGE_URI}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59066b59-4ea2-4c7b-ac38-a2d251e1b351",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Execute the script from CLI**\n",
    "\n",
    "```bash\n",
    "chmod +x push-to-gar.sh\n",
    "./push-to-gar.sh\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a4edc0-8a28-4ca4-813c-de87726a1b14",
   "metadata": {},
   "source": [
    "## Use the Image from Artifact Registry and a local training script to run a CustomJob on VertexAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b14011-d51b-4371-a16f-4b51290d7c19",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f19eee9-a1d9-4be3-9a2d-750bdd01cec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cd27a-5a48-43e3-89f5-2f3ca7bc0d32",
   "metadata": {},
   "source": [
    "### Define GCP Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749b2480-3e41-4d71-8717-43e2adf4d3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = \"project-id\"  # Set it properly\n",
    "\n",
    "# Region for launching jobs.\n",
    "REGION = \"us-central1\"  \n",
    "\n",
    "# Cloud Storage bucket for storing experiments output.\n",
    "BUCKET_URI = \"gs://gcp-notebook-test-bucket\"  \n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"model\")\n",
    "\n",
    "# Service Account for Launching Jobs\n",
    "SERVICE_ACCOUNT = \"compute@developer.gserviceaccount.com\"  # Set it properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423edec0-3192-4ef9-adc1-bad77ec2278c",
   "metadata": {},
   "source": [
    "### Define your training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06000661-0d10-4607-8981-692e17e041a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing finetune-gemma-lora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetune-gemma-lora.py\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    #Load dataset\n",
    "    raw_dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "    \n",
    "    # Define Model\n",
    "    model_id = \"google/gemma-2b\"\n",
    "\n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    \n",
    "    #Define Quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    \n",
    "    # LoRA config\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "    )\n",
    "\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=20,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    )\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        peft_config=peft_config,\n",
    "        args=training_args,\n",
    "        dataset_text_field=\"text\",\n",
    "        packing=True,\n",
    "        train_dataset=raw_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # save model\n",
    "    trainer.save_model()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e995ff-3afc-4ab6-b5af-fde9379d7059",
   "metadata": {},
   "source": [
    "### Launch Job using Vertex AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cee05a1-d1de-4504-a214-48f7fb9c6fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "job_name = get_job_name_with_datetime(\"gemma-lora-finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7869c0ca-0232-4bbd-ad8b-57bd0ff42c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Vertex AI SDK to store common configurations \n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b9e56f-db0c-44fd-a4df-bcc6f59c89cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Artifact Registry Image URI, that we pushed earlier\n",
    "TRAIN_DOCKER_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/deep-learning-images/huggingface-pytorch-training-gpu-2.1.transformers.4.38.1.py310:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319c7a40-9575-4f1d-825a-c9c4072f0d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finetune with 1 L4 (24G).\n",
    "machine_type = \"g2-standard-4\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "replica_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c24877-5d2b-43ea-8777-13847e64dad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://gcp-notebook-test-bucket/temporal/aiplatform-2024-03-15-14:46:36.960-aiplatform_custom_trainer_script-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "# Pass training arguments, training script, launch job using a local script.\n",
    "train_job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    script_path=\"finetune-gemma-lora.py\",\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    "    environment_variables={\"HF_TOKEN\": \"xxxx\"}, #Needed to download the model, Set it properly\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ad717-dd4c-4f3c-a63f-304ef9ea070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
