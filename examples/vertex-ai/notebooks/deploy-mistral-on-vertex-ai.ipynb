{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Mistral 7B on Vertex AI \n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-mistral-on-vertex-ai.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>\n",
    "\n",
    "[Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) is a large language model (LLM) developed by `Mistral AI` and is an instruct fine-tuned version of [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1).\n",
    "\n",
    "In this tutorial you will learn how to deploy [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) on Vertex AI Endpoints. We are going to use the Hugging Face [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) container which is a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "What you'll learn in this blog:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Configure gcloud CLI](#2-configure-gcloud-cli)\n",
    "3. [Initialize Vertex AI SDK](#3-initialize-vertex-ai-sdk)\n",
    "4. [Deploy Gemma-7B on Vertex AI](#4-deploy-gemma-7b-on-vertex-ai)\n",
    "5. [Run Inference with deployed Model](#5-run-inference-with-deployed-model)\n",
    "6. [Cleaning Up Resources](#6-cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `Vertex AI` python SDK to deploy [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) on Vertex AI. You need to have a GCP project and an account with the necessary permissions to create resources in the project. \n",
    "\n",
    "Before we can install the packages we need to install the `gcloud CLI`. Instructions can be found here: https://cloud.google.com/sdk/docs/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (4.39.0)\n",
      "Requirement already satisfied: filelock in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhamkrishna/.pyenv/versions/3.10.12/envs/venv-py310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages for the notebook\n",
    "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-storage \"google-auth>=2.23.3\"\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure gcloud CLI\n",
    "\n",
    "We need to authenticate with Google Cloud SDK to use the Vertex AI services. Run the following command to authenticate with Google Cloud SDK.\n",
    "\n",
    "```bash\n",
    "gcloud auth login \n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gcp-project-id\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "BUCKET_URI = f\"gs://vertexai-{PROJECT_ID}-tgi\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID} --quiet\n",
    "# Set the region\n",
    "! gcloud config set ai/region {REGION} --quiet\n",
    "# create the bucket if it doesn't exist\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Vertex AI SDK \n",
    "\n",
    "The following set of constants will be used to create names and display names of Vertex AI Prediction resources like models, endpoints, and model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model names and version\n",
    "MODEL_NAME = \"mistral-7b-hf\" # @param {type:\"string\"}\n",
    "MODEL_VERSION = \"v01\" # @param {type: \"string\"}\n",
    "MODEL_DISPLAY_NAME = f\"TGI-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = f\"endpoint-{MODEL_NAME}-{MODEL_VERSION}\" # @param {type:\"string\"}\n",
    "\n",
    "# Set the TGI serving container image uri, selected from https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face\n",
    "SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.1-4.ubuntu2204.py310\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Mistral-7B on Vertex AI\n",
    "\n",
    "To deploy [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) on Vertex AI, we first need to first upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/755607090520/locations/us-central1/models/3350890328507809792/operations/2342497462708600832\n",
      "Model created. Resource name: projects/755607090520/locations/us-central1/models/3350890328507809792@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/755607090520/locations/us-central1/models/3350890328507809792@1')\n",
      "TGI-mistral-7b-hf-v01\n",
      "projects/755607090520/locations/us-central1/models/3350890328507809792\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"mistralai/Mistral-7B-Instruct-v0.2\", # Hugging Face model ID\n",
    "        \"NUM_SHARD\": \"1\",\n",
    "        \"MAX_INPUT_LENGTH\": \"1512\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
    "        },\n",
    "    \n",
    "    serving_container_ports=[80],\n",
    ")\n",
    "\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is uploaded, now we can deploy it on Vertex AI Endpoints. \n",
    "First, we need to create an endpoint and then deploy the model to the endpoint. Here you also need to choose the hardware configuration for the deployment. \n",
    "\n",
    "The deployment will take ~20-25 minutes. You can check the status of the deployment in the Google cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/755607090520/locations/us-central1/endpoints/6650684152649613312/operations/3203247941489786880\n",
      "Endpoint created. Resource name: projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/755607090520/locations/us-central1/endpoints/6650684152649613312')\n",
      "Deploying model to Endpoint : projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n",
      "Deploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/6650684152649613312/operations/438037770284302336\n",
      "Endpoint model deployed. Resource name: projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n"
     ]
    }
   ],
   "source": [
    "machine_type = 'g2-standard-4' # L4 GPUs\n",
    "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
    "\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_NAME, # The display name of the deployed model\n",
    "    machine_type=machine_type,              # type of machine, read more here: https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "    accelerator_type=\"NVIDIA_L4\",           # Hardware accelerator \n",
    "    accelerator_count=1,                    # Number of accelerators to attach to a worker replica.\n",
    "    traffic_percentage=100,                 # Percentage of traffic to send to this model\n",
    "    min_replica_count=1,                    # The minimum number of machine replicas this deployed model will be always deployed on.\n",
    "    sync=True,                              # Whether to execute this method synchronously.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Inference with deployed Model\n",
    "Awesome! We have successfully deployed the Mistral-7B model on Vertex AI. Now let's run inference on our endpoint. We will use the `predict` method of the `Endpoint` class to run inference on the deployed model. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [TGI documentation](https://huggingface.github.io/text-generation-inference/) defined under the `GenerateParameters` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google Cloud is a suite of cloud computing services offered by Google. It provides a range of infrastructure and platform services for building, deploying, and scaling applications, websites, and services on the Google Cloud Platform (GCP). These services include computing power, storage options, networking, databases, machine learning, analytics, and more. Businesses and developers use Google Cloud to build, test, and deploy applications, process large amounts of data, and run workloads in a flexible, secure, and cost-effective way. Google Cloud also offers solutions for specific industries and use cases, such as healthcare, education, and finance. Additionally, Google Cloud provides tools for collaboration and productivity, such as Google Workspace (formerly known as G Suite).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"What is Google Cloud?\"},\n",
    "]\n",
    "\n",
    "res = deployed_model.predict(instances=[\n",
    "  {\"inputs\": tokenizer.apply_chat_template(messages, tokenize=False), \n",
    "   \"parameters\": {\"max_new_tokens\": 256, \"do_sample\": True, \"top_p\": 0.7, \"temparature\": 1.0 }} # Generation arguments\n",
    "  ]\n",
    ")\n",
    "print(res.predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up Resources\n",
    "\n",
    "To avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorial, you can delete the resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n",
      "Undeploy Endpoint model backing LRO: projects/755607090520/locations/us-central1/endpoints/6650684152649613312/operations/5711189983981731840\n",
      "Endpoint model undeployed. Resource name: projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n",
      "Deleting Endpoint : projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n",
      "Delete Endpoint  backing LRO: projects/755607090520/locations/us-central1/operations/7314471451325628416\n",
      "Endpoint deleted. . Resource name: projects/755607090520/locations/us-central1/endpoints/6650684152649613312\n",
      "Deleting Model : projects/755607090520/locations/us-central1/models/3350890328507809792\n",
      "Delete Model  backing LRO: projects/755607090520/locations/us-central1/models/3350890328507809792/operations/1549863928291393536\n",
      "Model deleted. . Resource name: projects/755607090520/locations/us-central1/models/3350890328507809792\n"
     ]
    }
   ],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
