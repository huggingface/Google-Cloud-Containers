{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate open LLMs with Vertex AI and Gemini\n",
    "\n",
    "The [Gen AI Evaluation Service in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) lets us evaluate LLMs or Application using existing or your own evaluation criterias. It supports academic metrics like BLEU, ROUGE, or LLM as a Judge with Pointwise and Pairwise metrics or custom metrics you can define yourself. As default LLM as a Judge `Gemini 1.5 Pro` is used.\n",
    "\n",
    "We can use the Gen AI Evaluation Service to evaluate the performance of open models and finetuned models using Vertex AI Endpoints and compute resources. In this example we will evaluate [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) generated summaries from news articles using a Pointwise metric based on [G-Eval](https://arxiv.org/abs/2303.16634) Coherence metric.\n",
    "\n",
    "We will cover the following topics:\n",
    "\n",
    "1. Setup / Configuration\n",
    "2. Deploy Llama 3.1 8B on Vertex AI\n",
    "3. Evaluate Llama 3.1 8B using different prompts on Coherence\n",
    "4. Interpret the results \n",
    "5. Clean up resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup / Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to install `gcloud` in your local machine, which is the command-line tool for Google Cloud, following the instructions at [Cloud SDK Documentation - Install the gcloud CLI](https://cloud.google.com/sdk/docs/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you also need to install the `google-cloud-aiplatform` Python SDK, required to programmatically create the Vertex AI model, register it, acreate the endpoint, and deploy it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet \"google-cloud-aiplatform[evaluation]\"  huggingface_hub transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use we define the following environment variables for GCP.\n",
    "\n",
    "_Note: Make sure to adapt the project ID to your GCP project._\n",
    "\n",
    "__Note: The Gen AI Evaluation Service is not available in all regions. If you want to use it, you need to select a region that supports it. At the moment of writing this example, only `us-central1` is supported._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROJECT_ID=gcp-partnership-412108\n",
    "%env LOCATION=us-central1\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to login into your GCP account and set the project ID to the one you want to use to register and deploy the models on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud auth application-default login  # For local development\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are logged in, you need to enable the necessary service APIs in GCP, such as the Vertex AI API, the Compute Engine API, and Google Container Registry related APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "!gcloud services enable container.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com\n",
    "!gcloud services enable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Llama 3.1 8B on Vertex AI\n",
    "\n",
    "Once everything is set up, we can deploy the Llama 3.1 8B model on Vertex AI. We will use the `google-cloud-aiplatform` Python SDK to do so. [`meta-llama/Meta-Llama-3.1-8B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) is a gated model, you need to login into your Hugging Face Hub account with a read-access token either fine-grained with access to the gated model, or just overall read-access to your account. More information on how to generate a read-only access token for the Hugging Face Hub in the instructions at <https://huggingface.co/docs/hub/en/security-tokens>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we are logged in we can \"upload\" the model i.e. register the model on Vertex AI. If you want to learn more about the arguments you can pass to the `upload` method, check out [Deploy Gemma 7B with TGI on Vertex AI](https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-gemma-on-vertex-ai/vertex-notebook.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=os.getenv(\"PROJECT_ID\"), location=os.getenv(\"LOCATION\"))\n",
    "\n",
    "endpoint_display_name = \"llama-3-1-8b-instruct-endpoint\"  # TODO: change to your endpoint display name\n",
    "\n",
    "# Iterates over all the Vertex AI Endpoints within the current project and keeps the first match (if any), otherwise set to None\n",
    "ENDPOINT_ID = next(\n",
    "    (endpoint.name for endpoint in aiplatform.Endpoint.list() \n",
    "     if endpoint.display_name == endpoint_display_name), \n",
    "    None\n",
    ")\n",
    "assert ENDPOINT_ID, (\n",
    "    \"`ENDPOINT_ID` is not set, please make sure that the `endpoint_display_name` is correct at \"\\\n",
    "    f\"https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={os.getenv('PROJECT_ID')}\"\n",
    ")\n",
    "\n",
    "deployed_model = aiplatform.Endpoint(f\"projects/{os.getenv('PROJECT_ID')}/locations/{os.getenv('LOCATION')}/endpoints/{ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=os.getenv(\"PROJECT_ID\"),\n",
    "    location=os.getenv(\"LOCATION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy the `meta-llama/Meta-Llama-3.1-8B-Instruct` to 1x NVIDIA L4 accelerator with 24GB memory. We set TGI parameters to allow for a maximum of 8000 input tokens, 8192 maximum total tokens, and 8192 maximum batch prefill tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_token\n",
    "\n",
    "vertex_model_name = \"llama-3-1-8b-instruct\"\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=vertex_model_name,\n",
    "    serving_container_image_uri=os.getenv(\"CONTAINER_URI\"),\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"MAX_INPUT_TOKENS\": \"8000\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": \"8192\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": get_token(),\n",
    "    },\n",
    "    serving_container_ports=[8080],\n",
    ")\n",
    "# model.wait() # wait for the model to be registered\n",
    "\n",
    "# # create endpoint\n",
    "# endpoint = aiplatform.Endpoint.create(display_name=f\"{vertex_model_name}-endpoint\")\n",
    "\n",
    "# # deploy model to 1x NVIDIA L4\n",
    "# deployed_model = model.deploy(\n",
    "#     endpoint=endpoint,\n",
    "#     machine_type=\"g2-standard-4\",\n",
    "#     accelerator_type=\"NVIDIA_L4\",\n",
    "#     accelerator_count=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: _The Vertex AI endpoint deployment via the `deploy` method may take from 15 to 25 minutes._\n",
    "\n",
    "After the model is deployed, we can test our endpoint. We generate a helper `generate` function to send requests to the deployed model. This will be later used to send requests to the deployed model and collect the outputs for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# grep the model id from the container spec environment variables\n",
    "# model_id = next((re.search(r'value: \"(.+)\"', str(item)).group(1) for item in list(model.container_spec.env) if 'MODEL_ID' in str(item)), None)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "generation_config = {\n",
    "  \"max_new_tokens\": 256,\n",
    "  \"do_sample\": True,\n",
    "  \"top_p\": 0.2,\n",
    "  \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "def generate(prompt, generation_config=generation_config):\n",
    "  formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "          {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "      )\n",
    "  \n",
    "  payload = {\n",
    "    \"inputs\": formatted_prompt,\n",
    "    \"parameters\": generation_config\n",
    "  }\n",
    "  output = deployed_model.predict(instances=[payload])\n",
    "  generated_text = output.predictions[0]\n",
    "  return generated_text\n",
    "\n",
    "\n",
    "generate(\"How many people live in Berlin?\", generation_config)\n",
    "# 'The population of Berlin is approximately 6.578 million as of my cut off data. However, considering it provides real-time updates, the current population might be slightly higher'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Llama 3.1 8B using different prompts on Coherence\n",
    "\n",
    "We will evaluate the Llama 3.1 8B model using different prompts on Coherence. Coherence measures how well the individual sentences within a summarized news article connect together to form a unified and easily understandable narrative.\n",
    "\n",
    "We are going to use the new [Generative AI Evaluation Service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview). The Gen AI Evaluation Service can be used to: \n",
    "* Model selection: Choose the best pre-trained model for your task based on benchmark results and its performance on your specific data.\n",
    "* Generation settings: Tweak model parameters (like temperature) to optimize output for your needs.\n",
    "* Prompt engineering: Craft effective prompts and prompt templates to guide the model towards your preferred behavior and responses.\n",
    "* Improve and safeguard fine-tuning: Fine-tune a model to improve performance for your use case, while avoiding biases or undesirable behaviors.\n",
    "* RAG optimization: Select the most effective Retrieval Augmented Generation (RAG) architecture to enhance performance for your application.\n",
    "* Migration: Continuously assess and improve the performance of your AI solution by migrating to newer models when they provide a clear advantage for your specific use case.\n",
    "\n",
    "In our case, we will use it to evaluate different prompt templates to achieve the most coherent summaries using Llama 3.1 8B Instruct. \n",
    "\n",
    "We are going to use a reference free Pointwise metric based on [G-Eval](https://arxiv.org/abs/2303.16634) Coherence metric. \n",
    "\n",
    "The first step is to define our prompt template and create our `PointwiseMetric`. Vertex AI returns our response from the model in the `response` field our news article will be made available in the `text` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalTask, PointwiseMetric\n",
    "\n",
    "g_eval_coherence = \"\"\"\n",
    "You are an expert evaluator. You will be given one summary written for a news article.\n",
    "Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the news article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{response}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- Coherence:\"\"\"\n",
    "\n",
    "metric = PointwiseMetric(\n",
    "    metric=\"g-eval-coherence\",\n",
    "    metric_prompt_template=g_eval_coherence,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use [argilla/news-summary](https://huggingface.co/datasets/argilla/news-summary) dataset consisting of news article from Reuters. We are going to use a random subset of 15 articles to keep the evaluation fast. Feel free to change the dataset and the number of articles to evaluate the model with more data and different topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subset_size = 15\n",
    "dataset = load_dataset(\"argilla/news-summary\", split=f\"train\").shuffle(seed=42).select(range(subset_size))\n",
    "\n",
    "# print first 150 characters of the first article\n",
    "print(dataset[0][\"text\"][:150])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run the evaluation, we need to convert our dataset into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns except for \"text\"\n",
    "to_remove = [col for col in dataset.features.keys() if col != \"text\"]\n",
    "dataset = dataset.remove_columns(to_remove)\n",
    "df = dataset.to_pandas()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We are almost ready. Last step is to define our different summarization prompts we want to use for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompts = {\n",
    "  \"simple\": \"Summarize the following news article: {text}\",\n",
    "  \"eli5\": \"Summarize the following news article in a way a 5 year old would understand: {text}\",\n",
    "  \"detailed\": \"\"\"Summarize the given news article, text, including all key points and supporting details? The summary should be comprehensive and accurately reflect the main message and arguments presented in the original text, while also being concise and easy to understand. To ensure accuracy, please read the text carefully and pay attention to any nuances or complexities in the language.\n",
    "  \n",
    "Article:\n",
    "{text}\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over our prompts and create different evaluation tasks, use our coherence metric to evaluate the summaries and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "results = {}\n",
    "for prompt_name, prompt in summarization_prompts.items():\n",
    "    prompt = summarization_prompts[prompt_name]\n",
    "\n",
    "    # 1. add new prompt column\n",
    "    df[\"prompt\"] = df[\"text\"].apply(lambda x: prompt.format(text=x))\n",
    "\n",
    "    # 2. create eval task\n",
    "    eval_task = EvalTask(\n",
    "        dataset=df,\n",
    "        metrics=[metric],\n",
    "        experiment=\"llama-3-1-8b-instruct\",\n",
    "    )\n",
    "    # 3. run eval task\n",
    "    # Note: If the last iteration takes > 1 minute you might need to retry the evaluation\n",
    "    exp_results = eval_task.evaluate(model=generate, experiment_run_name=f\"prompt-{prompt_name}-{str(uuid.uuid4())[:8]}\")\n",
    "    print(f\"{prompt_name}: {exp_results.summary_metrics['g-eval-coherence/mean']}\")\n",
    "    results[prompt_name] = exp_results.summary_metrics[\"g-eval-coherence/mean\"]\n",
    "\n",
    "for prompt_name, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{prompt_name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it looks like on our limited test the \"simple\" prompt yields the best results. We can inspect and compare the results in the GCP Console at [Vertex AI > Model Development > Experiments](https://console.cloud.google.com/vertex-ai/experiments).\n",
    "\n",
    "![experiment-results](./assets/experiment-results.png)\n",
    "\n",
    "The overview allows to compare the results across different experiments and to inspect the individual evaluations. Here we can see that the standard deviation of detailed is quite high. This could be because of the low sample size or that we need to improve the prompt further.\n",
    "\n",
    "You can find more examples on how to use the Gen AI Evaluation Service in the [Vertex AI Generative AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) including how to:\n",
    "* [how to customize the LLM as a Judge](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/bring_your_own_autorater_with_custom_metric.ipynb)\n",
    "* [how to use Pairwise metrics and compare different LLMs](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/compare_generative_ai_models.ipynb)\n",
    "* [how to evaluate different prompts more efficiently](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/prompt_engineering_gen_ai_evaluation_service_sdk.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can already release the resources that you've created as follows, to avoid unnecessary costs:\n",
    "\n",
    "* `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n",
    "* `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.\n",
    "* `model.delete` to delete the model from the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
