{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Gemma 7B from GCS with TGI on Vertex AI \n",
    "\n",
    "TL; DR Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models, developed by Google DeepMind and other teams across Google. Text Generation Inference (TGI) is a toolkit developed by Hugging Face for deploying and serving LLMs, with high performance text generation. And, Google Vertex AI is a Machine Learning (ML) platform that lets you train and deploy ML models and AI applications, and customize large language models (LLMs) for use in your AI-powered applications. In this example, we will show how to deploy any supported text-generation model, in this case [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it), downloaded from the Hugging Face Hub and uploaded to a Google Cloud Storage (GCS) Bucket, in Vertex AI using the TGI DLC available in Google Cloud Platform (GCP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![`google/gemma-7b-it` in the Hugging Face Hub](./assets/deploy-gemma-from-gcs-on-vertex-ai/model-in-hf-hub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup / Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install `gcloud` in our local machine, in order to be able to authenticate to Google Cloud, configure the project we want to use, our preferred / default location, etc.\n",
    "\n",
    "To install `gcloud`, follow the instructions at https://cloud.google.com/sdk/docs/install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will also need to install `google-cloud-aiplatform`, required to programatically create the Vertex AI model, register it in their model registry, and then create the endpoint to deploy the model in Vertex AI. To be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, for convenience we will set the following environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROJECT_ID=your-project-id\n",
    "%env LOCATION=your-location\n",
    "%env BUCKET_URI gs://hf-tgi-vertex-ai\n",
    "%env ARTIFACT_NAME google--gemma-7b-it\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu122.2-1-1.ubuntu2204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to login into our GCP account and set the project ID to the one we want to use for Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are logged in, we need to ensure that the necessary APIs are enabled in GCP, such as the Vertex AI, Compute Engine and Container Registry related APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "!gcloud services enable container.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com\n",
    "!gcloud services enable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Create bucket and upload model from Hub in GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you already have a GCS Bucket with the artifact that you want to serve, please follow the instructions below in order to create a new bucket and download and upload the model weights into it. Additionally, note that Vertex AI expects an `staging_bucket` to be provided, since there needs to be an existing GCS Bucket before registering and deploying any model in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start with the bucket creation, feel free to skip this section if you already have a bucket, otherwise, we will be using the `gcloud storage buckets create` command to create it under the current project and location.\n",
    "\n",
    "Additionally, we also need to install `gsutil`, since we will use it to check if the bucket already exists within GCS, to be installed as `gcloud components install gsutil`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Parse the bucket from the provided $BUCKET_URI path i.e. given gs://bucket-name/dir, extract bucket-name\n",
    "BUCKET_NAME=$(echo $BUCKET_URI | cut -d'/' -f3)\n",
    "# Check if the bucket exists, if not create it\n",
    "if [ -z \"$(gsutil ls | grep gs://$BUCKET_NAME)\" ]; then\n",
    "    gcloud storage buckets create gs://$BUCKET_NAME --project=$PROJECT_ID --location=$LOCATION --default-storage-class=STANDARD --uniform-bucket-level-access\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the bucket has been created (or if it was already created), we can upload the model artifact to it, which can either be done from an existing local model or directly from the Hugging Face Hub pulling the model locally and then uploading it to the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifact from disk / local storage\n",
    "\n",
    "So if the model is available locally in the say the path to the cached Hugging Face model in the `~/.cache/huggingface/hub/models--google--gemma-7b-it/snapshots/8adab6a35fdbcdae0ae41ab1f711b1bc8d05727e` directory, then we should run the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Upload the model to Google Cloud Storage\n",
    "LOCAL_DIR=~/.cache/huggingface/hub/models--google--gemma-7b-it/snapshots/8adab6a35fdbcdae0ae41ab1f711b1bc8d05727e\n",
    "if [ -d \"$LOCAL_DIR\" ]; then\n",
    "    gsutil -o GSUtil:parallel_composite_upload_threshold=150M -m cp -r $LOCAL_DIR/* $BUCKET_URI/$ARTIFACT_NAME\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifact from Hugging Face Hub\n",
    "\n",
    "Alternatively, if the  model is hosted in the Hugging Face Hub and not available locally, we will be using the `huggingface-cli` that needs to be installed follows, since we will be using `hf_transfer` to speed up the download process, and we also need to login into the Hugging Face Hub as we're trying to download a gated model that needs us to have access to in advance, which means that we should set the read permission token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"huggingface_hub[hf_transfer]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should run the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Ensure the necessary environment variables are set\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "\n",
    "# # Create a local directory to store the downloaded models\n",
    "LOCAL_DIR=\"tmp/google--gemma-7b-it\"\n",
    "mkdir -p $LOCAL_DIR\n",
    "\n",
    "# # Download models from HuggingFace, excluding certain file types\n",
    "huggingface-cli download google/gemma-7b-it --exclude \"*.bin\" \"*.pth\" \"*.gguf\" \".gitattributes\" --local-dir $LOCAL_DIR\n",
    "\n",
    "# Upload the downloaded models to Google Cloud Storage\n",
    "gsutil -o GSUtil:parallel_composite_upload_threshold=150M -m cp -e -r $LOCAL_DIR/* $BUCKET_URI/$ARTIFACT_NAME\n",
    "\n",
    "# Remove all files and hidden files in the target directory\n",
    "rm -rf tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the end to end script, please check [`./scripts/upload_model_to_gcs.sh`](https://github.com/huggingface/Google-Cloud-Containers/blob/main/scripts/upload_model_to_gcs.sh) within the root directory of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GCS Bucket with model artifact](./assets/deploy-gemma-from-gcs-on-vertex-ai/gcs-model-artifact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are logged in into our GCP account and enabled the required services, after installing `google-cloud-aiplatform` Python SDK, we can already initialize it using our previously defined `PROJECT_ID`, `LOCATION` and `BUCKET_URI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "LOCATION = os.getenv(\"LOCATION\")\n",
    "BUCKET_URI = os.getenv(\"BUCKET_URI\")\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=BUCKET_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can already proceed to the model \"upload\", since it will basically consist on registering the model in Vertex AI with an empty bucket linked to it, since we the model will be automatically downloaded in the Hugging Face TGI DLC as the `MODEL_ID` environment variable is provided.\n",
    "\n",
    "So on, before going into the code, let's review the arguments:\n",
    "\n",
    "- `display_name` is the name that will be shown in Vertex AI Model Registry.\n",
    "\n",
    "- `artifact_uri` is the path to the directory with the artifact within the GCS Bucket provided as `staging_bucket`. Note that it should contain a TGI-compatible model inside, since it will be copied into the container on start up, and later on, set as the `MODEL_ID` environment variable expected by the `text-generation-launcher`.\n",
    "\n",
    "- `serving_container_image_uri` is the location of the Hugging Face TGI DLC that we will be using for serving the model later on. In order to see which TGI containers are available in GCP, you can run the following command:\n",
    "\n",
    "    `gcloud container images list --repository=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io\" | grep \"huggingface-text-generation-inference\"`\n",
    "\n",
    "- `serving_container_environment_variables` are the environment variables that will be used during the container runtime, so these are aligned with the environment variables defined by TGI, which in this case natively supports the `AIP_` Vertex AI environment variables (to read more about the environment variables exposed by Vertex AI, please check https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements).\n",
    "    - `NUM_SHARD` is the number of shards to use if you don't want to use all GPUs on a given machine e.g. if you have two GPUs but you just want to use one for TGI then `NUM_SHARD=1`.\n",
    "    - `MAX_INPUT_TOKENS` is the maximum allowed input length (expressed in number of tokens), the larger it is, the larger the prompt can be, but also more memory will be consumed.\n",
    "    - `MAX_TOTAL_TOKENS` is the most important value to set as it defines the \"memory budget\" of running clients requests, the larger this value, the larger amount each request will be in your RAM and the less effective batching can be.\n",
    "    - `MAX_BATCH_PREFILL_TOKENS` limits the number of tokens for the prefill operation, as it takes the most memory and is compute bound, it is interesting to limit the number of requests that can be sent.\n",
    "    - `HUGGING_FACE_HUB_TOKEN` as we want to serve a gated model, `google/gemma-7b-it` in this case, we need to set the Hugging Face Hub token in advance in order to be able to access it from the TGI container. To generate a custom token for the Hugging Face Hub, you can follow the instructions at https://huggingface.co/docs/hub/en/security-tokens.\n",
    " \n",
    "    To read more about all the arguments supported by TGI, please visit https://huggingface.co/docs/text-generation-inference/main/en/basic_tutorials/launcher.\n",
    "\n",
    "- `serving_container_ports` this is optional, since we're setting this value to Vertex AI default port which is 8080, but is recommended to have more visibility on the open ports later on.\n",
    "\n",
    "For more information on the supported `aiplatform.Model.upload` arguments, check the `upload` reference at https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_URI = f\"{BUCKET_URI}/{os.getenv('ARTIFACT_NAME')}\"\n",
    "CONTAINER_URI = os.getenv(\"CONTAINER_URI\")\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"google--gemma-7b-it\",\n",
    "    artifact_uri=ARTIFACT_URI,\n",
    "    serving_container_image_uri=CONTAINER_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"NUM_SHARD\": \"1\",\n",
    "        \"MAX_INPUT_TOKENS\": \"512\",\n",
    "        \"MAX_TOTAL_TOKENS\": \"1024\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": \"1512\",\n",
    "    },\n",
    "    serving_container_ports=[8080],\n",
    ")\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model in Vertex AI Model Registry](./assets/deploy-gemma-from-gcs-on-vertex-ai/vertex-ai-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model in Vertex AI Model Registry with path to GCS](./assets/deploy-gemma-from-gcs-on-vertex-ai/vertex-ai-model-path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been registered in Vertex AI, we can define the endpoint we want to deploy the model to, and then link the model deployment to that endpoint resource.\n",
    "\n",
    "To do so, we'll start by calling `aiplatform.Endpoint.create` to create a new Vertex AI endpoint resource (which comes only with the configuration, it's not linked to a model or anythign usable yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=\"google--gemma-7b-it-endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vertex AI Endpoint created](./assets/deploy-gemma-from-gcs-on-vertex-ai/vertex-ai-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can already proceed to the model deployment in an endpoint via the `deploy` method within the previously registered `model`. The `deploy` method will link the previously created endpoint resource with the model that contains the configuration of the serving container, TEI in this case, and then it will deploy that model in Vertex AI in the specified instance/s.\n",
    "\n",
    "So on, before going into the code, let's review the arguments:\n",
    "\n",
    "- `endpoint` is the endpoint to deploy the model to, which is optional and by default will be set to the model's display name plus `_endpoint`; but in this case we're using a previously created endpoint.\n",
    "- `machine_type`, `accelerator_type` and `accelerator_count` are arguments that define which instance to use, and additionally, if desired, also the accelerator to use (GPU or TPU) and the number of accelerators, respectively. The `machine_type` and the `accelerator_type` are tied together, since when using an instance with an accelerator, we will need to select an instance that supports it, to read more about the different instances check https://cloud.google.com/compute/docs/gpus, and to read about the `accelerator_type` naming check https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec.\n",
    "- `sync` is an optional argument on whether to deploy the model and wait until it's done i.e. sync, or just trigger the deployment and continue the code execution i.e. async. In this case we set it to True (default value), as we won't be able to succesfully run the follow up cells until the endpoint is deployed.\n",
    "\n",
    "For more information on the supported `aiplatform.Model.deploy` arguments, check the `deploy` reference at https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: _The Vertex AI endpoint deployment via the `deploy` method may take from 15 to 25 minutes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vertex AI Endpoint running the model](./assets/deploy-gemma-from-gcs-on-vertex-ai/vertex-ai-endpoint-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vertex AI Endpoint logs in Cloud Logging](./assets/deploy-gemma-from-gcs-on-vertex-ai/vertex-ai-endpoint-logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run online inference in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the online predictions on Vertex AI using the `predict` method, which will basically send the requests to the running endpoint in predict route specified within the container. In order to do so, ideally we should first format the input query or conversation with the tokenizer that matches the model we're serving, being `google/gemma-7b-it`, and for that we will need to install `transformers` via `pip` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, the following snippet will apply the chat template to the input conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", token=os.getenv(\"HF_TOKEN\"))\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"What's Deep Learning?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the we send the formatted conversation as a single string to the TGI API via the `predict` method of the Vertex AI deployed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(\n",
    "    instances=[\n",
    "        {\n",
    "            \"inputs\": inputs,  # inputs = <bos><start_of_turn>user\\nWhat's Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 256, \"do_sample\": True,\n",
    "                \"top_p\": 0.95, \"temparature\": 1.0,\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which produces the following output:\n",
    "\n",
    "```\n",
    "Prediction(predictions=['Deep Learning is a type of machine learning that uses artificial neural networks (ANNs) to learn from large amounts of data.\\n\\n**Key Concepts:**\\n\\n* **Neural Networks:** Multilayered structure that mimics the interconnected neurons in the brain.\\n* **Deep Learning Architectures:** Complex neural network models designed for specific tasks, such as image recognition, Natural Language Processing (NLP), or speech recognition.\\n* **Data:** Massive amounts of labeled and unlabeled data used for training and testing deep learning models.\\n* **Backpropagation:** Algorithm for adjusting weights in neural networks to optimize performance.\\n* **Transfer Learning:** Leveraging pre-trained deep learning models for new tasks.\\n\\n**Types of Deep Learning:**\\n\\n* **Supervised Learning:** Models learn from labeled data, where inputs have corresponding outputs.\\n* **Unsupervised Learning:** Models learn from unlabeled data, seeking patterns or structure.\\n* **Reinforcement Learning:** Models learn through trial and error by interacting with an environment.\\n\\n**Applications:**\\n\\n* Image Recognition\\n* Natural Language Processing (NLP)\\n* Speech Recognition\\n* Fraud Detection\\n* Autonomous Systems\\n\\n**Benefits:**\\n\\n* **High Accuracy:** Deep learning models can achieve high accuracy on complex tasks.\\n* **Automation:**'], deployed_model_id='2590503271110017024', metadata=None, model_version_id='1', model_resource_name='projects/755607090520/locations/us-central1/models/6909974283246632960', explanations=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also use the Online Prediction UI within the Vertex AI endpoint from the \"Test your model\" preview feature, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vertex AI Endpoint online inference](./assets/deploy-gemma-on-vertex-ai/vertex-ai-online-prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can release the resources we have created as follows:\n",
    "\n",
    "- `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n",
    "- `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully after the `undeploy_all`.\n",
    "- `model.delete` to delete the model from the registry i.e. unregister it. Note that when using a Google Cloud Storage (GCS) artifact, this method won't delete neither the bucket nor its contents, but only unregister the model from Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can also remove the GCS Bucket we created before, with the following `gcloud` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
