{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740ec248",
   "metadata": {},
   "source": [
    "# Finetune LLaMa-2 (7B) on Vertex AI\n",
    "\n",
    "\n",
    "Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters.\n",
    "\n",
    "In this tutorial you will learn how to finetune [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) on Vertex AI. \n",
    "\n",
    "\n",
    "What you'll learn in this tutorial:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Fine-tune Llama-2-7b using `trl` and `SFTTrainer`](#3-fine-tune-llama2-7b-using-trl-and-sfttrainer)\n",
    "4. [Inference with Fine-tuned Model](#4-inference-with-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea87eb",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "\n",
    "In this example, we will use the Vertex AI Workbench instance with A100 and the [Hugging Face Deep Learning Containers](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face). The Hugging Face PyTorch DLC comes with all important libraries, like Transformers, Datasets, PEFT, TRL and other packages pre-installed this makes it super easy to get started, since there is no need for environment management. You can now find all Hugging Face containers on [Google Cloud](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face).\n",
    "\n",
    "\n",
    "**ToDo**: Add info on how to spin-up a workbench instance or small intro about Vertex AI Workbench Instance.\n",
    "\n",
    "**ToDo**: Update the link for the image once, GPU containers are released. \n",
    "\n",
    "\n",
    "Once the instance is up and running, we can access a Jupyter environment, which we can use for preparing our dataset and launching the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a0500",
   "metadata": {},
   "source": [
    "## 2. Load the dataset \n",
    "\n",
    "We use the [Abirate/english_quotes](https://huggingface.co/datasets/Abirate/english_quotes) dataset, is a dataset of all the quotes retrieved from [goodreads quotes](https://www.goodreads.com/quotes). This dataset can be used for multi-label text classification and text generation. The content of each quote is in English and concerns the domain of datasets for NLP and beyond.\n",
    "\n",
    "An example from the dataset:\n",
    "```python\n",
    "{\n",
    "   \"quote\": \"Be yourself; everyone else is already taken.\"\t\n",
    "   \"author\": \"Oscar Wilde\"\n",
    "    \"tags\": [ \"be-yourself\", \"gilbert-perreira\", \"honesty\", \"inspirational\", \"misattributed-oscar-wilde\", \"quote-investigator\" ]\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `Abirate/english_quotes` dataset, we use the load_dataset() method from the ü§ó Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e3d84",
   "metadata": {},
   "source": [
    "To fine-tune our model we need to convert our structured examples into a format that is supported by the SFTTrainer. We  define a `format_dataset` function that concats the `quote` and `author` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ef48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(sample):\n",
    "    sample[\"text\"] = f\"Quote: {sample['quote']}\\nAuthor: {sample['author']}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0d2d2",
   "metadata": {},
   "source": [
    "Before applying formatting on our entire dataset, lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3221fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dataset(dataset[randrange(len(dataset))])['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4de921",
   "metadata": {},
   "source": [
    "We can see that the dataset was properly formatted as the `quote` and `author` information has been appended into `text` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f25cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply formatting\n",
    "dataset = dataset.map(\n",
    "    format_dataset, remove_columns=list(dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931be2d9",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama-2-7b using `trl` and `SFTTrainer`\n",
    "\n",
    "We will use the [SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer) from  ü§ó `trl` to fine-tune our model. The `SFTTrainer`  is built on top of the ü§ó Transformers `Trainer` and inherits all the core functionalities like logging, evaluation, and checkpointing, but offers additional enhancements like:\n",
    "\n",
    "- Packing datasets for more efficient training\n",
    "- PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
    "- Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    "\n",
    "You can read about it in the [trl docs](https://huggingface.co/docs/trl/en/sft_trainer)\n",
    "\n",
    "\n",
    "As, we all know LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. Therefore, we  are going to use [QLoRA](https://arxiv.org/abs/2106.09685), a technqiue technique to reduce the memory footprint of LLMs during finetuning, without sacrificing performance. How it works: \n",
    "\n",
    "- Quantize the pretrained model to 4 bits and freezing it.\n",
    "- Attach small, trainable adapter layers. (LoRA)\n",
    "- Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "To further enhance training efficiency, we'll incorporate a recently introduced, high-performance attention mechanism `Flash Attention 2` alongside `QLoRA`. It is nicely integrated with Transformers. It is up to 3x faster than the standard attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b264f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     #  quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\",             # use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_use_double_quant=True,        # use a nested quantization scheme to quantize the already quantized weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    ")                                          # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a4732",
   "metadata": {},
   "source": [
    "In order to use `meta-llama/Llama-2-7b-hf` you will need the Hugging Face Hub Token with access to the model, so make sure to execute the following:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login # The easiest way to authenticate and it saves the token on your machine. \n",
    "```\n",
    "\n",
    "There are other ways too which can be found in the [docs](https://huggingface.co/docs/huggingface_hub/en/quick-start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\",\n",
    "                                             attn_implementation=\"flash_attention_2\"\n",
    "                                            )\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059afdce",
   "metadata": {},
   "source": [
    "For using QLoRA with SFTTrainer, we need to create our LoraConfig and pass it as an argument to the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c85a0f",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"output\",               # directory to save trained model\n",
    "    max_steps = 100,                      # number of training steps\n",
    "    learning_rate = 2e-4,                # learning rate for training\n",
    "    optim=\"paged_adamw_8bit\",            # optimizer for training\n",
    "    per_device_train_batch_size = 1,     # batch size per device during training\n",
    "    gradient_accumulation_steps = 4,     # Number of steps to accumulate gradients before updating the model\n",
    "    logging_steps = 10,                   # log every 2 steps\n",
    "    bf16 = True                          # Use float16 when running on a GPU(T4, V100) where bfloat16 is not supported\n",
    "                                         # conversion from bfloat16 to float16 may lead to overflow (and opposite may lead to loss of precision)                                       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b6191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the trl SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", # field that contains the text in the dataset\n",
    "    args = training_args,\n",
    "    peft_config = peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe818667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a01cfa",
   "metadata": {},
   "source": [
    "## 4. Inference with Fine-tuned Model\n",
    "\n",
    "Once the fine-tuning is done, we want to run inference on the fine-tuned model. We utilize some prompts from the original dataset and see how does the text generation using the fine-tuned model looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee42801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "## Load the adapted model\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(model, \"output\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6becf",
   "metadata": {},
   "source": [
    "Select some prompts for text generation and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Quote: ‚ÄúBe yourself; everyone else is already taken.‚Äù\",\n",
    "    \"Quote: ‚ÄúGood friends, good books, and a sleepy conscience: this is the ideal life.‚Äù\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.autocast(device):\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=20)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae049fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde0428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
